I"ä<p>Reinforcement learning covers a family of algorithms with the purpose of maximize a cumulative reward that an <em>agent</em> can obtain from an <em>environment</em>.
It seems like <a href="http://www.thecrowbox.com/">training crows</a> to collect cigarette butts in exchange for peanuts, or paraphrasing an old say, the carrot and stick metaphor for cold algorithms instead of living donkeys.</p>

<p>The <em>agent</em> and <em>environment</em> have not been emphasized vainly, they represent more concretely a vacuum cleaner
sweeping your flat, an A/B testing engine for commerce or a driveless car in a crossroad. If you have heard about latest advances in the field, you would have came across of Deepmind‚Äôs <a href="https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/">AlphaZero</a>, by which it is possible, with an affordable set of hardware, to build from scratch the <a href="https://www.chess.com/news/view/updated-alphazero-crushes-stockfish-in-new-1-000-game-match">best chess player</a> in the world in just 4 hours.</p>

<iframe src="//www.slideshare.net/slideshow/embed_code/key/vpPWyEJugR0VVp" width="795" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""> </iframe>

<blockquote>
  <p>‚ÄúDifficulties strengthen the mind, as labor does the body.‚Äù
‚Äï <em>Lucius Annaeus Seneca</em></p>
</blockquote>

<p>Researches don‚Äôt lack of challenges in this field. The most important one comes from the intrinsic nature of RL learning process, which rely solely on the evaluations of its actions. Improvements are driven by just one signal, the reward.</p>

<blockquote>
  <p>Rewards are often <em>very sparse</em>.</p>
</blockquote>

<p>They come after hundred or thousands of steps, exponentially increasing the combination of actions the agent must explore for finding a barely better sequence among of them. If that does not seem arduous, consider also the <em>non-stationary</em> nature of some environments, particularly common in dynamic scenarios where the learning phase resemble pursuing a moving target.</p>

<h3 id="non-stationary-environments">Non-stationary environments</h3>
<p>Non-stationary means that the return of an action, performed in the precisely exact conditions of a past experience, might be different from what expected. This is particularly intuitive in the case of multi-agent scenario (<a href="http://www.dcsc.tudelft.nl/~bdeschutter/pub/rep/10_003.pdf">MARL</a>), in which the agent plays with one or more other agents that are learning too.</p>

<h3 id="what-distinguish-rl-from-other-optimization-methods">What distinguish RL from other optimization methods</h3>
<p>While RL helps on creating agents that can autonomously take decisions, other algorithms attain this goal too, but with different working principles.</p>

<p><em>Supervised learning</em> could be easily distinguished because it is trained with correct samples instead of vague rewards, making it simpler for <a href="https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23">loss functions</a> to converge into an useful solution.</p>

<p><em>Mathematical optimization</em> differs from RL in a more subtle way. Likewise RL, the <a href="https://en.wikipedia.org/wiki/Simplex_algorithm">simplex algorithm</a> find solutions by iterating on optimization loops, but it works only on perfect information problems.
When considering what it exactly means, let‚Äôs look at the <a href="https://en.wikipedia.org/wiki/Knapsack_problem">knapsack</a> or the <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">traveling salesman</a> problems.</p>
<blockquote>
  <p>All the necessary informations for elaborating the optimal solution are readily there.</p>
</blockquote>

<p>In other words, there is no exploration.
Conversely, an RL agent is like a probe on an heavenly body, where the assumptions on the environment are nearly absent. The agent needs to figure out autonomously the good and the bad actions only by the feedback from environment, the so called <em>model free</em> learning approach.</p>

<p><em>Genetic Programming</em> is an evolutionary optimization method that share most of the characteristics of RL, it is iterative, suitable for imperfect information systems due to its stochastic explorative nature. Even the terminology is somehow related. For example, what is the objective function, in <em>GP</em> is named <em>fitness function</em>, just a polyseme.
What differentiate it from RL is it‚Äôs evolutionary method, I briefly explained in <a href="/2018/06/28/first-steps-evolutionary/">this post</a>.</p>
:ET