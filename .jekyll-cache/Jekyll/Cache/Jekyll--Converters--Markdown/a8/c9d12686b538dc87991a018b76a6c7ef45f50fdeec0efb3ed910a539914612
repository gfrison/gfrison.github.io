I"•<p>In the Divine Comedy, Minos is a daemon appointed to guard the entrance of the hell. He listens to the sins of souls
and indicates them their destinations by wrapping his tail as many times as the assigned circle.
The figure is emblematic of the machine learning classification, where an entity is identified as belonging to
a category or to another. Rather than condemning souls to endless pains, the harmless tool I am describing can judge whether an userâ€™s utterance belongs to a specific intention, or to a limited range of emotions. Namely, it can serve intention recognition and sentimental analysis.</p>

<p>In the realm of conversational commerce, the examined sentence could be:</p>

<p><code class="highlighter-rouge">I want to buy some apples and pears</code></p>

<p>The system recognizes the intention <code class="highlighter-rouge">search</code> and presents the results.</p>

<p>Intention prediction is not an untackled problem and the market offers plenty of services.
There are many players such as Google (Api.ai), Facebook (Wit.ai), Microsoft (Luis.ai) just for mentioning some of them,
but this shouldnâ€™t prevent further explorations in the topic, sometimes with unexpected positive surprises, as shows in the graph.</p>

<p><img src="http://localhost:4000/assets/minos-chart.png" alt="Minos Accuracy" /></p>

<p>The test was performed against real data used for training the deployed model of the Chatbot system and the results are relevant for the real working scenario, so no <em>cherry picking</em> in this case. 300 training samples, 56 test samples for 25 classes, these are the datasetâ€™s numbers.</p>

<p>Minos, the text classifier, uses an ensemble of machine learning models. It combines multiple classifiers for getting a good prediction out of utterances submitted to <a href="https://www.facebook.com/charlygrocery/">Charly</a>.
One of the models is based on Convolutional Neural Networks (CNN).</p>

<h2 id="cnn-in-nlp">CNN in NLP</h2>

<p>CNN is mostly applied to image recognition thanks to the tollerance on translations
(rotations, distortions) and the compositionality principle (entities are composed by its constituents).
Admittedly, CNN might appear counter-intuitive at a first approach because text looks very different from images:</p>
<ol>
  <li>The order of the words in text is not as important as the order of the pixel in an image.</li>
  <li>Humans percept text sequentially, not in convolutions.</li>
</ol>

<h3 id="invariance">Invariance</h3>

<p>Entities like images and texts, should be compared differently. The smallest atomic element in text is the single charater, rather than the word, like the pixel in images. The proportion is more like:</p>

<p><code class="highlighter-rouge">text : char = image : pixel</code></p>

<p>By this angle of view, the order of characters in sentences is fundamental. Convolutions in text come in form of:</p>

<p><code class="highlighter-rouge">single word</code> =&gt; <code class="highlighter-rouge">bi-grams (two adjacent words)</code> =&gt; <code class="highlighter-rouge">n-grams</code></p>

<p>like graphical features</p>

<p><code class="highlighter-rouge">lines , corners</code> =&gt; <code class="highlighter-rouge">mouths, eyes</code> =&gt; <code class="highlighter-rouge">faces</code></p>

<p>come out of portraits.</p>

<p>In CNN the pair <code class="highlighter-rouge">adjective + object</code> for example,
could be recognized invariantly of its position, at the begin or at the end of a sentence, exactly like a face is recognized wherever it is located in the whole picture.</p>

<h3 id="sequentiality">Sequentiality</h3>

<p>It might seem more intuitive to apply Recurrent Neural Networks (like LSTM, Attention or Seq2seq) for text classification,
due to the sequential nature of RNNs algorithms. I didnâ€™t run any test on them so far, but I would promptly play with <a href="http://arxiv.org/abs/1503.00075">TreeLSTM</a>. CNN performs well, and one might say that <em>Essentially, all models are wrong, but some are useful</em>, an essay the fit with the idea that final outcome drives the decisions, and experimental results play an important role.</p>

<h3 id="word-embeddings">Word Embeddings</h3>

<p>Alike any NLP, in CNN words are replaced by their correspective semantic vector. Most famous are Google word2vec, GloVe and FastText.
I decided to make use of <a href="https://arxiv.org/abs/1704.03560">ConceptNet Numberbatch</a>
that took first place in both subtasks at <a href="http://alt.qcri.org/semeval2017/task2/">SemEval 2017 task 2</a>.
Moreover, the vector file is very small (~250M) compared to Google News word2vec (~1.5G) and from an engeneering point of view, those numbers matter.</p>

<p>Minos is still experimental and not well tuned, doors are open for improvements. An aspect shouldnâ€™t be ignored on working with CNN is the <a href="https://arxiv.org/abs/1612.00796">Catastrofic Forgetting</a>, an annoying phenomenon that ruins irrevocably the entire training.</p>
:ET