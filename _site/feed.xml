<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-10-15T14:13:26+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Discontinuities</title><subtitle></subtitle><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><entry><title type="html">Machine Learning and BigData will make Precision Medicine the future of Healthcare</title><link href="http://localhost:4000/2019/09/27/precision-medicine-healthcare/" rel="alternate" type="text/html" title="Machine Learning and BigData will make Precision Medicine the future of Healthcare" /><published>2019-09-27T00:00:00+00:00</published><updated>2019-09-27T00:00:00+00:00</updated><id>http://localhost:4000/2019/09/27/precision-medicine-healthcare</id><content type="html" xml:base="http://localhost:4000/2019/09/27/precision-medicine-healthcare/">&lt;p&gt;According to an &lt;a href=&quot;https://www.bmj.com/content/353/bmj.i2139&quot;&gt;article&lt;/a&gt; published by the British Medical Journal in 2013, medical errors are the third cause of death in the US, and  it is &lt;a href=&quot;https://qualitysafety.bmj.com/content/22/Suppl_2/ii21&quot;&gt;prudent assumption&lt;/a&gt; that wrong diagnosis would have a strong incidence in errors that can be harmful to patients. 10-15% of diagnoses are wrong and 35% of adults experienced a medical mistake in the past 5 years involving themselves, their family, or friends. Without considering extreme consequences, the chance to receive a false diagnosis is almost certain along the entire span of life.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;originally posted on https://cxlabs.sap.com/2019/06/18/data-analytics-for-precision-medicine/&lt;/em&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;http://localhost:4000/assets/causes-of-death.png&quot; /&gt;&lt;/center&gt;
&lt;p&gt;It’s worth taking a moment to consider why these errors happen in the first place. There are more than 12.000 diseases according to the World Health Organization. Never heard of foreign accent syndrome or alien hand syndrome? Many doctors wouldn’t have either, they’re just two of the many new diseases that are constantly emerging. Among the causes of mistakes, there are the variability of how the disease might manifest. There are doctor variables such as expertise. We know also that errors occur either as a result of &lt;a href=&quot;https://www.amjmed.com/article/S0002-9343%2808%2900040-5/fulltext&quot;&gt;physicians’ overconfidence&lt;/a&gt;. In the current era of machine learning and big data would be attainable to offer decision-support tools to fix the faulty synthesis of information? May as well be inspiring to look on how the automotive industry adopt those technologies for car maintenance.&lt;/p&gt;

&lt;p&gt;Modern cars have many features for ensuring assistance and safety to drivers and passengers. The vehicles’ onboard computer make thousands of decisions based on hundreds of sensors, that among their purposes, perform diagnosis by monitoring internal signals for evidence of faults. Terabytes of data are processed under the hood for real-time analytics, and not just for audit the current state of the car, but also for suggesting the best treatment before real problems occur. In the long term, this can reduce the cost burden of maintenance. Could also predictive diagnostic save human lives by detecting diseases early and precisely even before they appear?&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;http://localhost:4000/assets/snyderome.jpg&quot; /&gt;&lt;/center&gt;
&lt;p&gt;This what Prof. Michael Snyder &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/22424236&quot;&gt;performed&lt;/a&gt; to himself, when he combined high-throughput technologies during 14 months of screenings. Why standard medical evaluations examine only a handful biomedical markers when the technology (in the far 2013) measured up to 40.000 elements and potentially learn a lot more? His paper reports more than 3 billion measurements (50TB of data) over infected and healthy states.  Doing so, it is not only possible to identify physiological statuses that deviate from the healthy state but also do it long before the symptoms appear. Exactly like in your car’s monitoring system.&lt;/p&gt;

&lt;p&gt;The concept of precision medicine (PM) hinges on one key premise: most diagnostics are designed for the “average patient” as a “one-size-fits-all” approach. But there is no such “average patient” and thus most treatments will be successful for some patients but not for others. Instead, therapies will be shaped to classes of patients on the basis of the differences in people’s genes, molecular information, clinical data, diet, environment, lifestyle and other biomarkers. Prof. Snyder felt that genomics “saved his life” because he found out about the diabetes early and he was able to make lifestyle changes, even though his health insurance premium became “prohibitively expensive”. However, he says the trade-off was worth it.&lt;/p&gt;

&lt;p&gt;Precision medicine has emerged as a computationally approach to interpret the big data of pools of biological molecules (omics) and facilitate their application in healthcare. But doesn’t come without critics. One of them regards costs. Complete genomic sequencing became faster and less expensive with the introduction of new methods. However, reimbursement from insurance companies for these targeted screenings is likely to be still an issue. One of the most imposing barriers of PM may be the willingness and motivation of doctors to incorporate routine genomic testing into their daily workflows while the clinical benefits have not been definitively established.&lt;/p&gt;

&lt;p&gt;Algorithmically, there is a shift to using informatics methods such as neural networks and advanced aggregative techniques to model complex relationships among patients within gene and biomolecules signatures to facilitate this process.
In the survey &lt;a href=&quot;https://go.oracle.com/LP=66444?elqCampaignId=95819&quot;&gt;Trends in Precision Medicine Adoption&lt;/a&gt; made by GenomeWeb and sponsored by Oracle, the majority of respondents believe PM brings competitive advantages to their organizations while one of the most significant obstacles in pursuing PM is the lack of the information technology infrastructure to support the initiative.
Interpreting the amount of data collected by medical screenings and also by wearable devices (the so-called Digital Biomarkers) for multiple individuals would cause a structural bottleneck, that could be alleviated by a correspondent shift towards streaming and big data processing, a challenge that could be transformed into opportunity by first-class information providers.&lt;/p&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html">According to an article published by the British Medical Journal in 2013, medical errors are the third cause of death in the US, and it is prudent assumption that wrong diagnosis would have a strong incidence in errors that can be harmful to patients. 10-15% of diagnoses are wrong and 35% of adults experienced a medical mistake in the past 5 years involving themselves, their family, or friends. Without considering extreme consequences, the chance to receive a false diagnosis is almost certain along the entire span of life.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/dna.jpg" /></entry><entry><title type="html">Photography Event in Bassano del Grappa</title><link href="http://localhost:4000/2019/09/08/photo-exposition-bassano-fotografia/" rel="alternate" type="text/html" title="Photography Event in Bassano del Grappa" /><published>2019-09-08T00:00:00+00:00</published><updated>2019-09-08T00:00:00+00:00</updated><id>http://localhost:4000/2019/09/08/photo-exposition-bassano-fotografia</id><content type="html" xml:base="http://localhost:4000/2019/09/08/photo-exposition-bassano-fotografia/">&lt;p&gt;I’ve been since long time fascinated by photography and its power to capture my fantasy and attention. I was pretty delighted
to attend artistic initiatives, that by crossing the boundaries of museums and ateliers, they can reach unforeseen spectators in very unconventional settings.
The “FuoriSalone” in Milano, for example, is a majestic example of that. Arts, design and springing creativity flourish in areas
where people are used to see anything but design masterpieces. Similarly, in one of the cutest towns on the foot of alps, Bassano del Grappa,
is going to &lt;a href=&quot;http://www.bassanofotografia.it/&quot;&gt;take place&lt;/a&gt; a photography event that will cover the historic center with splendid images.&lt;/p&gt;

&lt;p&gt;I’m honored to give a small contribution, by exposing some pictures of mine in a couple of remarkable sites of the town: the main square (Piazza Liberta’) with its solar clock
 and Viale dei Martiri, where visitors can admire the view of the mountains and the medieval towers.&lt;/p&gt;

&lt;h2 id=&quot;san-marco-caffe&quot;&gt;San Marco Caffe’&lt;/h2&gt;
&lt;p&gt;The Caffe’ is &lt;a href=&quot;https://goo.gl/maps/fEBrUCyXhznyANbB6&quot;&gt;located in the main square&lt;/a&gt;, and visitors can see a glimpse of pictures even from the Piazza Liberta’.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Our curse is that we are forced to interpret life as a sequence of events and that when we can’t figure out what our particular story is we feel lost somehow.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The quote reflects somehow the experiment I performed during the Faschingsdienstag (Carnival Tuesday) in München:&lt;/p&gt;
&lt;center&gt;&lt;img title=&quot;Giancarlo Frison - Pin-ups&quot; src=&quot;http://localhost:4000/assets/marienplatz3.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img title=&quot;Giancarlo Frison - Skates&quot; src=&quot;http://localhost:4000/assets/skate.jpg&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;les-amis&quot;&gt;Les Amis&lt;/h2&gt;
&lt;p&gt;Along the Viale dei Martiri, a &lt;a href=&quot;https://goo.gl/maps/VuQTRVZxU3WmVoKUA&quot;&gt;romantic shop&lt;/a&gt; with clothing for woman is hosting a couple of photos taken during an open
concert in the Nymphemburg Park (München), &lt;em&gt;Serenade im Park&lt;/em&gt;, few summers ago:&lt;/p&gt;
&lt;center&gt;&lt;img title=&quot;Giancarlo Frison - Skates&quot; src=&quot;http://localhost:4000/assets/serenade-im-park1.jpg&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img title=&quot;Giancarlo Frison - Skates&quot; src=&quot;http://localhost:4000/assets/serenade-impark2.jpg&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Any preference on those pictures?&lt;/p&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html">I’ve been since long time fascinated by photography and its power to capture my fantasy and attention. I was pretty delighted to attend artistic initiatives, that by crossing the boundaries of museums and ateliers, they can reach unforeseen spectators in very unconventional settings. The “FuoriSalone” in Milano, for example, is a majestic example of that. Arts, design and springing creativity flourish in areas where people are used to see anything but design masterpieces. Similarly, in one of the cutest towns on the foot of alps, Bassano del Grappa, is going to take place a photography event that will cover the historic center with splendid images.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/GiancarloFrison-bf19-1.jpg" /></entry><entry><title type="html">Dynamic Programming</title><link href="http://localhost:4000/2019/06/18/dynamic-programming/" rel="alternate" type="text/html" title="Dynamic Programming" /><published>2019-06-18T00:00:00+00:00</published><updated>2019-06-18T00:00:00+00:00</updated><id>http://localhost:4000/2019/06/18/dynamic-programming</id><content type="html" xml:base="http://localhost:4000/2019/06/18/dynamic-programming/">&lt;p&gt;The term &lt;em&gt;dynamic programming&lt;/em&gt; has a curious origin.
When Richard Bellman late in the 1940s were seeking for a viral definition of his method, his boss was apparently not very inclined on science and in particular on mathematical research, the activities that Bellman was deep into when he formulate his famous &lt;a href=&quot;https://en.wikipedia.org/wiki/Bellman_equation&quot;&gt;equation&lt;/a&gt;.
Likewise in marketing campaigns, where names and terms are carefully selected for attention teasing, Bellman coined the definition that combines the multi-staging and time-varying of &lt;em&gt;‘dynamic’&lt;/em&gt; with the optimization archetyping of &lt;em&gt;‘programming’&lt;/em&gt;, coincidentally induced by the fame of Dantzig’s &lt;em&gt;linear programming&lt;/em&gt; for mathematical optimization.&lt;/p&gt;
&lt;figure&gt;
  &lt;a href=&quot;https://www.flickr.com/photos/abbiateci64/32615368572&quot;&gt;&lt;img title=&quot;Umberto Boccioni - Rissa in Galleria(1910) by Maurizio Abbiateci (CCBY2)&quot; src=&quot;http://localhost:4000/assets/rissa-galleria.jpg&quot; /&gt;&lt;/a&gt;
  &lt;figcaption&gt;Umberto Boccioni - Rissa in Galleria(1910) by Maurizio Abbiateci (CCBY2)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;DP&lt;/em&gt; describes problems that involve dynamic processes for finding the best decision one after another. The backbone idea rolls around the splitting of a problem till its atomic parts are identified, then trimming those parts with a objective function. Basically, the core concept of &lt;em&gt;recursion&lt;/em&gt;. Considering for example the Fibonacci algorithm:&lt;/p&gt;

&lt;center&gt;&lt;img title=&quot;Fibonacci&quot; src=&quot;http://localhost:4000/assets/fib-tree.png&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The node &lt;code class=&quot;highlighter-rouge&quot;&gt;fib(2)&lt;/code&gt; appear twice in the tree, therefore Fibonacci unveils an &lt;em&gt;overlapping&lt;/em&gt; structure. It is eligible to be an &lt;em&gt;DP&lt;/em&gt; algorithm. Not all recursive algorithms are inherently overlapping, look at &lt;em&gt;binary tree search&lt;/em&gt;, &lt;em&gt;merge&lt;/em&gt; and &lt;em&gt;quick sort&lt;/em&gt;, for example. They don’t manifest the property of traversing smaller chunk of data more then once, hence they cannot join Dynamic Programming’s family. When the sub-problems are repeated in the problem as whole, and their values are evaluated all over again unless a caching mechanism (&lt;em&gt;memoization&lt;/em&gt;) could be displaced for preventing such inefficiency.&lt;/p&gt;

&lt;p&gt;Bellman also conceived the &lt;em&gt;principle of optimality&lt;/em&gt; according to which an optimal policy should always hands out the optimal decision from any state or action previously done. However the chessboard pieces’ are laid, or however pedestrian are crossing the road or waiting on the platform, the agent will move pieces properly in case of chess and it will skirt traversing people in case of self-driving car. The automatic actor will always follow the best strategy from the first step and thereafter.
This is for introducing another property of &lt;em&gt;DP&lt;/em&gt; methods, which is the &lt;em&gt;optimal structure&lt;/em&gt;.&lt;/p&gt;

&lt;center&gt;&lt;img title=&quot;shortest path problem&quot; src=&quot;http://localhost:4000/assets/shortest-path.png&quot; /&gt;&lt;/center&gt;

&lt;p&gt;When a courier has to deliver a packet from &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; to the destination &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;, he will catch the shortest way highlighted in blue. The optimal decision for completing the journey &lt;script type=&quot;math/tex&quot;&gt;A \rightarrow F&lt;/script&gt;, in the shortest path problem, includes necessarily the optimal solution &lt;script type=&quot;math/tex&quot;&gt;C \rightarrow F&lt;/script&gt;. As you may have noticed, The recursion sweats also out of this property where nested sub-problems are always optimal on their way to the final target. The shortest path problem present the &lt;em&gt;optimal sub-structure&lt;/em&gt;, which claims that an optimal solution of a problem includes necessarily the optimal solutions of its sub-problems. Not all recursive problems are optimal even in their encapsulated structure, and the counterfactual is given by slightly different dilemma, the longest path problem. The longest way &lt;script type=&quot;math/tex&quot;&gt;A \rightarrow F&lt;/script&gt; includes the node &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;, but if we choose to start from &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;, the path is not included in the way starting from &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-programming-in-reinforcement-learning&quot;&gt;Dynamic programming in reinforcement learning&lt;/h2&gt;
&lt;p&gt;The first time &lt;em&gt;DP&lt;/em&gt; and &lt;em&gt;RL&lt;/em&gt; were mentioned together was by Minsky in the 1961 and it took form of the Bellman equation, because &lt;em&gt;RL&lt;/em&gt; problems have expose usually an overlapping and optimal structure, ideal for being solved by &lt;em&gt;DP&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&quot;markov-decision-process&quot;&gt;Markov decision process&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;/2018/12/26/first-steps-reiforcement-learning/&quot;&gt;Reinforcement learning&lt;/a&gt; is a class of methods for determining the optimal policy an agent should apply for maximize its return in a given environment. The entities with a role in &lt;em&gt;RL&lt;/em&gt; are the state &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;, the mutable conditions an agent experience in an environment, the action &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; the agent execute in a particular state, and the reward &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;, if any, which score the goodness of action taken in a state. These are the pillars of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;&gt;markov decision process&lt;/a&gt;, the framework that formalizes the policy &lt;script type=&quot;math/tex&quot;&gt;π&lt;/script&gt; as a sequence of steps &lt;script type=&quot;math/tex&quot;&gt;(S_t, A_t, R_t)&lt;/script&gt;, aggregated in episodes.&lt;/p&gt;

&lt;center&gt;&lt;img title=&quot;MDP&quot; src=&quot;http://localhost:4000/assets/mdp-states.png&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The &lt;em&gt;policy&lt;/em&gt; is a mapping from states and related actions, &lt;em&gt;RL&lt;/em&gt; tells us how the agent’s policy changes as result of the experience.
For finding good policies, we need to estimate how good it is, in terms of future rewards, to be in a particular state.  Value functions &lt;script type=&quot;math/tex&quot;&gt;v_\pi(s)&lt;/script&gt; define the expected return when starting from a given state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and following &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; thereafter. their fundamental property is that they satisfy recursive relationships similar to what we already have seen for dynamic programming. Hence, the idea of &lt;em&gt;DP&lt;/em&gt; in &lt;em&gt;RL&lt;/em&gt; is the use of value functions to organize the search for good policies.&lt;/p&gt;

&lt;h3 id=&quot;the-gridworld-example&quot;&gt;The Gridworld example&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/gridworld.png&quot; alt=&quot;GridWorld&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Problem: Find all trajectories towards the flags from any cell&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Among many examples I may show for explaining what does &lt;em&gt;DP&lt;/em&gt; stands for, this very simple &lt;em&gt;GridWorld&lt;/em&gt; conveys the idea of an agent displaced in an 2D environment. The reward drives the agent to move toward the flagged corners regardless the position where the agent is located.
How the agent could learn the optimal ways? I give you an hint, let’s start from the end, the flagged boxes.
In that position you don’t have anymore rewards to gather, the task is brilliantly completed, so we assume the terminal’s state value is is equal to &lt;em&gt;zero&lt;/em&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/mdp-t1.png&quot; alt=&quot;GridWorld&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Iteration 1. Terminal state value is zero&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;From end position, step back and look around. Among the actions you can do, the move that catapult you into the terminal state is more rewarding than others (&lt;em&gt;zero&lt;/em&gt; instead of &lt;em&gt;-1&lt;/em&gt;).
Congratulations! You already have solved a piece of the puzzle. It is just the last action, you still have to solve the rest of the grid. In that position, as in any other cell of the grid, you need to assign a value to the cell where you are in that particular moment, in order to pave the way to the final goal. That value &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; indicates how good it is to be there. Not all cells carry the same value, some are more valuable than others. The agent, when choosing the next move, will move to the cell with highest value, among the surroundings. The value would not be just the reward you get in there (&lt;em&gt;-1&lt;/em&gt;), but the reward plus the average of the values of the cells proximate to you.
&lt;script type=&quot;math/tex&quot;&gt;Q_t = r_t + \sum_{n=t+1}{Q_n}&lt;/script&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/mdp-t2.png&quot; alt=&quot;GridWorld&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Iteration 2: set the terminal state value to _zero_&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Going backward till the initial position, you see all rewards on the way to the terminal state waiting for you to being picked up. This is the value of your state.
You are in the middle of the trajectory towards the end, and the value of your state is equal of the reward in that state plus the sum of the discounted rewards thereafter.
Clearly, the agent moves toward the most promising among the surrounding cells, once it has realized their value.  Let’s keep going.
The grid starts to unveil the optimal trajectories by just going backward and evaluating what could be the best move, taking the cell with highest value.
A pattern is identifiable, something that programmers knows well, the &lt;em&gt;recursion&lt;/em&gt;, and simplified as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_t=r_t+argmax(Q_{t+1})&lt;/script&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html">The term dynamic programming has a curious origin. When Richard Bellman late in the 1940s were seeking for a viral definition of his method, his boss was apparently not very inclined on science and in particular on mathematical research, the activities that Bellman was deep into when he formulate his famous equation. Likewise in marketing campaigns, where names and terms are carefully selected for attention teasing, Bellman coined the definition that combines the multi-staging and time-varying of ‘dynamic’ with the optimization archetyping of ‘programming’, coincidentally induced by the fame of Dantzig’s linear programming for mathematical optimization. Umberto Boccioni - Rissa in Galleria(1910) by Maurizio Abbiateci (CCBY2)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/rissa-galleria.jpg" /></entry><entry><title type="html">First steps in Reinforcement Learning</title><link href="http://localhost:4000/2018/12/26/first-steps-reiforcement-learning/" rel="alternate" type="text/html" title="First steps in Reinforcement Learning" /><published>2018-12-26T00:00:00+00:00</published><updated>2018-12-26T00:00:00+00:00</updated><id>http://localhost:4000/2018/12/26/first-steps-reiforcement-learning</id><content type="html" xml:base="http://localhost:4000/2018/12/26/first-steps-reiforcement-learning/">&lt;p&gt;Reinforcement learning covers a family of algorithms with the purpose of maximize a cumulative reward that an &lt;em&gt;agent&lt;/em&gt; can obtain from an &lt;em&gt;environment&lt;/em&gt;.
It seems like &lt;a href=&quot;http://www.thecrowbox.com/&quot;&gt;training crows&lt;/a&gt; to collect cigarette butts in exchange for peanuts, or paraphrasing an old say, the carrot and stick metaphor for cold algorithms instead of living donkeys.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;agent&lt;/em&gt; and &lt;em&gt;environment&lt;/em&gt; have not been emphasized vainly, they represent more concretely a vacuum cleaner
sweeping your flat, an A/B testing engine for commerce or a driveless car in a crossroad. If you have heard about latest advances in the field, you would have came across of Deepmind’s &lt;a href=&quot;https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/&quot;&gt;AlphaZero&lt;/a&gt;, by which it is possible, with an affordable set of hardware, to build from scratch the &lt;a href=&quot;https://www.chess.com/news/view/updated-alphazero-crushes-stockfish-in-new-1-000-game-match&quot;&gt;best chess player&lt;/a&gt; in the world in just 4 hours.&lt;/p&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/vpPWyEJugR0VVp&quot; width=&quot;795&quot; height=&quot;485&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Difficulties strengthen the mind, as labor does the body.”
― &lt;em&gt;Lucius Annaeus Seneca&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Researches don’t lack of challenges in this field. The most important one comes from the intrinsic nature of RL learning process, which rely solely on the evaluations of its actions. Improvements are driven by just one signal, the reward.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Rewards are often &lt;em&gt;very sparse&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They come after hundred or thousands of steps, exponentially increasing the combination of actions the agent must explore for finding a barely better sequence among of them. If that does not seem arduous, consider also the &lt;em&gt;non-stationary&lt;/em&gt; nature of some environments, particularly common in dynamic scenarios where the learning phase resemble pursuing a moving target.&lt;/p&gt;

&lt;h3 id=&quot;non-stationary-environments&quot;&gt;Non-stationary environments&lt;/h3&gt;
&lt;p&gt;Non-stationary means that the return of an action, performed in the precisely exact conditions of a past experience, might be different from what expected. This is particularly intuitive in the case of multi-agent scenario (&lt;a href=&quot;http://www.dcsc.tudelft.nl/~bdeschutter/pub/rep/10_003.pdf&quot;&gt;MARL&lt;/a&gt;), in which the agent plays with one or more other agents that are learning too.&lt;/p&gt;

&lt;h3 id=&quot;what-distinguish-rl-from-other-optimization-methods&quot;&gt;What distinguish RL from other optimization methods&lt;/h3&gt;
&lt;p&gt;While RL helps on creating agents that can autonomously take decisions, other algorithms attain this goal too, but with different working principles.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Supervised learning&lt;/em&gt; could be easily distinguished because it is trained with correct samples instead of vague rewards, making it simpler for &lt;a href=&quot;https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23&quot;&gt;loss functions&lt;/a&gt; to converge into an useful solution.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Mathematical optimization&lt;/em&gt; differs from RL in a more subtle way. Likewise RL, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Simplex_algorithm&quot;&gt;simplex algorithm&lt;/a&gt; find solutions by iterating on optimization loops, but it works only on perfect information problems.
When considering what it exactly means, let’s look at the &lt;a href=&quot;https://en.wikipedia.org/wiki/Knapsack_problem&quot;&gt;knapsack&lt;/a&gt; or the &lt;a href=&quot;https://en.wikipedia.org/wiki/Travelling_salesman_problem&quot;&gt;traveling salesman&lt;/a&gt; problems.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;All the necessary informations for elaborating the optimal solution are readily there.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, there is no exploration.
Conversely, an RL agent is like a probe on an heavenly body, where the assumptions on the environment are nearly absent. The agent needs to figure out autonomously the good and the bad actions only by the feedback from environment, the so called &lt;em&gt;model free&lt;/em&gt; learning approach.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Genetic Programming&lt;/em&gt; is an evolutionary optimization method that share most of the characteristics of RL, it is iterative, suitable for imperfect information systems due to its stochastic explorative nature. Even the terminology is somehow related. For example, what is the objective function, in &lt;em&gt;GP&lt;/em&gt; is named &lt;em&gt;fitness function&lt;/em&gt;, just a polyseme.
What differentiate it from RL is it’s evolutionary method, I briefly explained in &lt;a href=&quot;/2018/06/28/first-steps-evolutionary/&quot;&gt;this post&lt;/a&gt;.&lt;/p&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html">Reinforcement learning covers a family of algorithms with the purpose of maximize a cumulative reward that an agent can obtain from an environment. It seems like training crows to collect cigarette butts in exchange for peanuts, or paraphrasing an old say, the carrot and stick metaphor for cold algorithms instead of living donkeys.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/rl-survey-giancarlo-frison.png" /></entry><entry><title type="html">Una Stella in più</title><link href="http://localhost:4000/2018/10/26/una-stella-in-piu/" rel="alternate" type="text/html" title="Una Stella in più" /><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><id>http://localhost:4000/2018/10/26/una-stella-in-piu</id><content type="html" xml:base="http://localhost:4000/2018/10/26/una-stella-in-piu/">&lt;center&gt;&lt;img title=&quot;Mirella Bobbo&quot; src=&quot;http://localhost:4000/assets/mamma.jpg&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Se fossi la prima a morire &lt;br /&gt;
non lasciare che il dolore oscuri il tuo cielo.&lt;br /&gt;
Sii coraggiosa e modesta nel tuo lutto.&lt;br /&gt;
È un cambiamento, non un addio.&lt;br /&gt;
Così i morti vivono nei vivi&lt;br /&gt;
e tutte quelle piccole cose&lt;br /&gt;
raccolte nel viaggio della tua vita,&lt;br /&gt;
quelle parole semplici ed umili&lt;br /&gt;
dettate dal cuore di una mamma sofferente,&lt;br /&gt;
sono una ricchezza da conservare gelosamente.&lt;br /&gt;
Nel tuo cuore.”&lt;br /&gt;&lt;br /&gt;
&lt;em&gt;Mirella Bobbo&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hai affrontato la morte più e più volte. Sei stata offesa nel corpo, condannata ad un letto, ad una carrozzina ed alla solitudine.
Mi hai insegnato con parole umili e con la tua caparbietà a non mollare mai.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“La mia sofferenza è silenziosa dentro&lt;br /&gt;
dentro nell’anima.&lt;br /&gt;
Invece fuori parla&lt;br /&gt;
parla con il sorriso dell’amicizia,&lt;br /&gt;
e profuma di umili piante e fiori di campo.&lt;br /&gt;
La mia sofferenza non cerca la pietà&lt;br /&gt;
ma la forza d’animo.&lt;br /&gt;
E questa forza mi viene data da persone sofferenti.&lt;br /&gt;
Senti che il dolore se diviso&lt;br /&gt;
pian piano se ne va&lt;br /&gt;
lasciando un’infinita gioia e felicità.“&lt;br /&gt;&lt;br /&gt;
&lt;em&gt;Mirella Bobbo&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Inesorabile è stato il logorio del tempo contro cui a nessuno è concesso rivaleggiare. A poco a poco, ti ha tolto la forza, ma non ha mai spento la fiamma che avevi in te. Sei stata una leonessa, di dolcezza e sensibilità.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Signore, dammi abbastanza lacrime&lt;br /&gt;
per mantenermi umana&lt;br /&gt;
i sorrisi per conservarmi ottimista&lt;br /&gt;
dammi le sconfitte per mantenermi umile&lt;br /&gt;
abbastanza successo per mantenermi fiduciosa&lt;br /&gt;
gli amici per fondermi coraggio&lt;br /&gt;
i ricordi per darmi conforto&lt;br /&gt;
abbastanza pazienza per sostenermi nell’attesa&lt;br /&gt;
la speranza per accompagnarmi nell’incertezza.&lt;br /&gt;
Aiutami a scoprire i tuoi messaggi&lt;br /&gt;
Nella realtà che vivo &lt;br /&gt;
triste realtà.&lt;br /&gt;
Ti prego, non rendermi mai esigente&lt;br /&gt;
da pretendere ciò che io vorrei&lt;br /&gt;
ma permettimi di ringraziarti per ciò che tu vorrai donarmi.“&lt;br /&gt;&lt;br /&gt;
&lt;em&gt;Mirella Bobbo&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Mi raccontavi di quando per poco, per davvero poco, sei sfuggita da un riposo senza ritorno, grazie alla tempestiva rianimazione dei medici.
Ricordavi il momento di pace, il sollievo dai dolori lancinanti che avevi in testa. Per un attimo hai creduto di essere libera e serena. Per fortuna invece, ci hai regalato molto più tempo di quanto la vita voleva darti.&lt;/p&gt;

&lt;p&gt;Sembrava quasi ti svegliassi a momenti, quando ti ho rivista ieri sera.
Quel momento di pace e quiete è arrivato e domenica scorsa ti sei assopita ed hai lasciato questo mondo, in pace.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Se mi sveglierò filo d’erba sarò felice&lt;br /&gt;
perché tutto il cielo sarà mio.&lt;br /&gt;
Se mi sveglierò alito di vento sarò felice&lt;br /&gt;
perché sentirete il mio respiro.&lt;br /&gt;
Se mi sveglierò pioggia sarò felice&lt;br /&gt;
perché disseterò la terra ed il grano crescerà orgoglioso&lt;br /&gt;
per il pane di ogni dí.&lt;br /&gt;
Se mi sveglierò in un grande giardino&lt;br /&gt;
coperto di fiori profumati, dove i bambini corrono felici&lt;br /&gt;
sarò felice&lt;br /&gt;
perché sarò finalmente in paradiso.“&lt;br /&gt;&lt;br /&gt;
&lt;em&gt;Mirella Bobbo&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Mi manca la tua gioia nel rivedermi. Il tuo sorriso sarà sempre con noi.&lt;/p&gt;

&lt;p&gt;Ovunque tu sia, ti voglio bene mamma.&lt;/p&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/mamma.jpg" /></entry><entry><title type="html">Program Induction and Synthesis at ICML 2018</title><link href="http://localhost:4000/2018/08/02/program-induction-synthesis-icml-2018/" rel="alternate" type="text/html" title="Program Induction and Synthesis at ICML 2018" /><published>2018-08-02T00:00:00+00:00</published><updated>2018-08-02T00:00:00+00:00</updated><id>http://localhost:4000/2018/08/02/program-induction-synthesis-icml-2018</id><content type="html" xml:base="http://localhost:4000/2018/08/02/program-induction-synthesis-icml-2018/">&lt;p&gt;&lt;img src=&quot;https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/06/images.jpeg&quot; alt=&quot;&quot; /&gt;
The International Conference on Machine Learning &lt;a href=&quot;https://icml.cc&quot;&gt;ICML&lt;/a&gt; took place this year in Europe,
in the beautiful city of Stockholm from 10th to 15th of July.
This is one of the two premiere conferences (within NIPS) on Artificial Intelligence research, and the numbers indicate the magnitude of the event: 612 accepted papers out of 2473 submissions, 9 tutorial and 67 workshop sessions on the latest advances in all disciplines of machine learning. One of the most intriguing workshop was about machine intelligence capable of writing software code for complex procedural behavior.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://labs.hybris.com/2018/08/28/program-induction-and-synthesis-at-icml-2018/&quot;&gt;see full article&lt;/a&gt;&lt;/p&gt;

&lt;!--
## Program Induction

Can we teach computers to write code? This is the question that brings out an entire branch of research specialized in program synthesis. Programming is a demanding task that requires extensive knowledge, experience and not a frivolous degree of creativity. Despite the premises that discourage any form of automation, machine learning can reshape the way software is developed. That could be seen as a giant step comparable to the transition of punch cards in the _'60s_ in favor of magnetic tapes. New programming systems could enable non-programmers to produce correct, cheap, safe and efficient software, opening new automation horizons even for not yet foreseeable purposes.

Curious on what does it feel like? Try to describe a problem in plain English and translate it in a routine in your favorite programming language. Let's have a look on [this sample](https://arxiv.org/abs/1807.03168):
&gt; You are given a number var0. You have to set var2 to 2. If var0-2 is divisible by 3 you have to set var1 to 1,
otherwise you have to set var1 to zero. For each var3 between 1 and var0-1, if var2 is less than var0 you have to, add var3*3+2 to var2, if var0-var2 is greater than or equal to zero and var0-var2 is divisible by 3 add 1 to var1;
otherwise you have to break from the enclosing loop. You have to return var1.

As you might have noticed, this is a detailed description of an algorithm. Natural language lacks of precise semantics necessary for describing concepts with mathematic granularity, this is why the generated code looks less amusing than the original _(natural)_ code:
&lt;script src=&quot;https://gist.github.com/a4e43b1d6833af9c94b00ab97645f9ab.js&quot;&gt; &lt;/script&gt;


Likewise ML tasks detect patterns, a program synthesis task aims to solve problems that might be identified as a composition of basic programming primitives, such as conditional operators, loop controls and even [recursions](https://arxiv.org/abs/1704.06611) and used for generating specific program
[trees](/2018/06/28/first-steps-evolutionary/).

&lt;center&gt;&lt;img title=&quot;Program Induction&quot; src=&quot;http://localhost:4000/assets/pinduction-schema.png&quot;/&gt;&lt;/center&gt;

Generating code is a extremely challenging problem and the output's goodness is still very limited by using existing approaches.
One discrepancy in the flow above settle in the different nature of the encoded solution (generally neural networks) and generated code.
Neural networks lay in the differentiable realm (gradient-based training) while source code generation belongs to the discrete parish, because of its rigorous grammar and intolerance to typos.
If you want to get hands dirty on it, it is now available a [public dataset](https://near.ai/research/naps/) for training your system on software programming.

An introductory talk on this matter was hold by [Joshua Tenenbaum](https://www.csail.mit.edu/person/joshua-tenenbaum), professor at MIT and contributor of [Bayesian methods](https://www.researchgate.net/publication/2463513_A_Bayesian_Framework_for_Concept_Learning) for computational learning. He is specialized in cognitive sciences and and he strives to grasp the fundamental mechanics to human learning, in order to ultimately transfer them to computer programs. I find it useful to describe the Bayesian approach as a generic framework for every-day life decisions, where possible solutions are weighted according to our observations and past experiences flavored by doses of uncertainty.

[![DreamCoder](http://localhost:4000/assets/dreamcoder.png)](http://www.youtube.com/watch?v=RB78vRUO6X8?t=3389)

The [DreamCoder](https://uclmr.github.io/nampi/extended_abstracts/ellis.pdf) (Ellis 2018) rumbles on the idea of creating a repository of simple problems (and their paired solutions) for being reused on solving more complex ones. Even though the full paper is so far not publicly available, it seems inspired by the ways programmers organize their work: building shared subroutines that can be composed to develop more complex procedures. Instead of solving all problems from scratch, it tries to think flexibly and brings what it has already learned.

### Program synthesis using example

What I guess would be the most promising and attainable utilization of program induction is coding repairing and migration. Raise an hand 🙋 who hates repetitive, error-prone, edits due to general refactoring or library upgrading. Those tasks occurs during software evolution and they can be done only manually, since they are beyond the capabilities of IDEs. [REFAZER](http://www.dsc.ufcg.edu.br/~spg/refazer/) (rebuild in Portuguese) attempts to classify code transformations by large set of examples. It has been proven to be correct on *84%* of the transformations, which is not bad at all. The research has been published and it is available [here](https://people.eecs.berkeley.edu/~bjoern/papers/rolim-refazer-icse2017.pdf) (Rolim et.al; ICSE 2017). The technique for synthesizing programs from examples quotes:
&gt;Each rewrite rule matches some subtrees of the given AST and outputs modified
versions of these subtrees. Additionally, we specify constraints
for our DSL operators based on the input-output examples to
reduce the search space of transformations, allowing [PROSE](https://microsoft.github.io/prose/) to
efficiently synthesize them.  

 There are large and open repository for such dataset, take for example Github as widely known repository for open-sourced projects, so uniquely for this case, the training data availability should not be a blocking issue.
--&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><category term="program" /><category term="induction," /><category term="icml," /><category term="machine" /><category term="learning" /><summary type="html">The International Conference on Machine Learning ICML took place this year in Europe, in the beautiful city of Stockholm from 10th to 15th of July. This is one of the two premiere conferences (within NIPS) on Artificial Intelligence research, and the numbers indicate the magnitude of the event: 612 accepted papers out of 2473 submissions, 9 tutorial and 67 workshop sessions on the latest advances in all disciplines of machine learning. One of the most intriguing workshop was about machine intelligence capable of writing software code for complex procedural behavior.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/dreamcoder.png" /></entry><entry><title type="html">First Steps on Evolutionary Systems</title><link href="http://localhost:4000/2018/06/28/first-steps-evolutionary/" rel="alternate" type="text/html" title="First Steps on Evolutionary Systems" /><published>2018-06-28T00:00:00+00:00</published><updated>2018-06-28T00:00:00+00:00</updated><id>http://localhost:4000/2018/06/28/first-steps-evolutionary</id><content type="html" xml:base="http://localhost:4000/2018/06/28/first-steps-evolutionary/">&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/evolution.jpg&quot; alt=&quot;Nick Youngson CC BY-SA 3.0 Alpha Stock Images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Goal programming attempts to find solutions which possibly satisfy, otherwise violates minimally,  a set of goals. It has been enjoyed in innumerable domains such as engineering, financing or resource allocation. Solutions may include optimal strategies to maximize, for example, a sale’s profit or, on the other hand, to minimize the cost of a purchase under an acceptable threshold.&lt;/p&gt;

&lt;p&gt;An optimized plan could be blended as a program defined as an abstract syntax tree (AST):&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;http://localhost:4000/assets/ast.png&quot; title=&quot;Abstract Syntax Tree&quot; width=&quot;300&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;a href=&quot;https://labs.hybris.com/2018/07/02/first-steps-on-evolutionary-systems/&quot;&gt;see full article&lt;/a&gt;&lt;/p&gt;

&lt;!--

This is the tree representation of $$\frac{a}{10}+4b$$. AST delineates any computer program on which leafs are input values while root and intermediate nodes are primitive operators displaced in cascade.
An automatic system built upon goal optimizations should develop methods for synthesizing programs by running intelligent agents that learn by them self on how to reach targets. Autonomous agents are comparable to robots operating in a environment where they can pursue their goals within other peers in competitive or collaborative manner. For the level of complexity of a multi-agent system, one of the promising technique could be found, is in the realm of evolutionary algorithms.

Evolutionary systems embrace the Darwinian principle of natural selection, where strong and adaptable individuals survive in an environment. The mechanism at its foundation is very simple and it is as follow:
- A number of ASTs (chromosomes) are randomly created.
- Each chromosome is evaluated through a fitness function.
- Best ones are selected, the others are disposed.
- Chromosomes could be breded among the selected for a new generation.
- Offsprings are randomly mutated.
- Repeat until the score threshold is reached.

The “breeding” is called crossover. Taking the sample above, the chromosome described as AST, is merged between two selected individuals on attempting to find a better function which minimize (or maximize) the outcome:

&lt;center&gt;&lt;img src=&quot;http://localhost:4000/assets/ast-crossover.png&quot; title=&quot;Genetic Programming Crossover&quot; width=&quot;400&quot;/&gt;&lt;/center&gt;

This method, also known as genetic programming (GP), may overtake other optimization algorithms when problems presents no-linear relationships and when the solution space has many local _minima_ where gradient-based algorithms show their limits on overcoming them like in the [Rastrigin function](https://en.wikipedia.org/wiki/Rastrigin_function):

&lt;center&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/8/8b/Rastrigin_function.png&quot; width=&quot;400&quot; title=&quot;Rastrigin function&quot;/&gt;&lt;/center&gt;

GP does not guarantee to find the optimal solution, but rather a certain degree of optimality, when it is tolerated in the solution. GP might appear to brute force the seeking for solutions, but the cumulative selection lower the complexity to very few generations, like reported in the [Weasel program](https://en.wikipedia.org/wiki/Weasel_program):
&gt;I don't know who it was first pointed out that, given enough time, a monkey bashing away at random on a typewriter could produce all the works of Shakespeare. The operative phrase is, of course, given enough time. Let us limit the task facing our monkey somewhat. Suppose that he has to produce, not the complete works of Shakespeare but just the short sentence 'Methinks it is like a weasel', and we shall make it relatively easy by giving him a typewriter with a restricted keyboard, one with just the 26 (capital) letters, and a space bar. How long will he take to write this one little sentence?

GP has [been proved](https://arxiv.org/abs/1712.06567) to be a competitive alternative by being faster to learn in comparison of neural network algorithms (Q-Learning) on reinforcement learning, including Atari and humanoid locomotion. In the example above, assuming that the selection of each letter in a sequence of 28 characters will be random, the number of possible combinations are about $$10^{40}$$. GP solves it in 46 generations.

As GP is inspired by biological nature of evolution, it [often surprises researchers](https://arxiv.org/abs/1803.03453) by unexpected outcomes. That is the case of a software created for repairing buggy code. It found a clever loophole in order to fix a bug in a sorting algorithm:
&gt;In other experiments, the fitness function rewarded minimizing the difference between what the program generated and the ideal target output, which was stored in text files. After several generations of evolution, suddenly and strangely, many perfectly fit solutions appeared, seemingly out of nowhere. Upon manual inspection, these highly fit programs still were clearly broken. It turned out that one of the individuals had deleted all of the target files when it was run!

## Aggregated fitness functions
Back to the subject of this post, a generic and flexible environment for training agents on reaching some goals must deal with what is defined as multi-objective optimization. The final outcome should solve several goals which might be in conflict with each other, like for example growing profit for a business and rise salary to its employees. Multi-objective optimization give rise to a set of [Pareto-optimal](https://en.wikipedia.org/wiki/Pareto_efficiency) solutions. The purpose of training is to create agents that can find as many such solutions as possible. The aggregated fitness function (AFF) has minimal knowledge on how a goal is achieved and evaluates only _what_ is actually achieved. Therefore, the procedure of how an agent accomplishes a task is irrelevant. The drawback is obviously that there is no guidance for evolution through immediate solutions.

## Simple experiment
A GP system has been instructed to model as many ASTs as the number of digit of a randomly generated set of numbers. Those programs should find the respective digit out of the given number, like units, tens and so on. Assuming $$P$$ as a set of $$n$$ programs, and $$D$$ the the digits of the input number when $$D=46$$ the output will be $$P_1(D)=4$$ and $$P_2(D)=6$$, just as simple as that. The fitness function measure the square of the sum of the distance between the predicted and real digit. The fitness function returns the sum of the squared error between the calculated digit $$P(D)$$ and the correct ones $$d$$. In doing so, the feedback for the single program is lost, only the aggregated one is considered by the genetic algorithm.

$$err = \frac{\sum_{i=1}^n (D_i - P_i(D))^2}{\sum_{i=1}^n D_i} $$

&lt;center&gt;&lt;img src=&quot;http://localhost:4000/assets/evolution-graph.png&quot;  title=&quot;Multi-Objective Genetic Programming&quot;/&gt;&lt;/center&gt;

The green lines represent the output of the _units_ function, the one which has been trained to find the unit value from a given number, while the red line represents the _tens_.
--&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/evolution-graph.png" /></entry><entry><title type="html">The Basic Principles of Language</title><link href="http://localhost:4000/2018/06/13/basic-principles-language/" rel="alternate" type="text/html" title="The Basic Principles of Language" /><published>2018-06-13T00:00:00+00:00</published><updated>2018-06-13T00:00:00+00:00</updated><id>http://localhost:4000/2018/06/13/basic-principles-language</id><content type="html" xml:base="http://localhost:4000/2018/06/13/basic-principles-language/">&lt;p&gt;What is this exhilarating noise come out of my mouth when I talk? Not surely because that precise sequence of sounds, pops and squeezes are particularly melodic, but thanks to that palace of sophistications erected in favor of language, we can talk and afford a wide range of expressions. Since I began erratically to explore natural language processing I have been wondering how it comes out so natural for us, while it is extremely complicated from a computational perspective. What has caught my curiosity is the nature of language and its fundamental aspects that might have shaped the rudimentary &lt;em&gt;‘Me Tarzan, you Jane’&lt;/em&gt;, the sentence that paraphrases the earliest and the simplest level of language.&lt;/p&gt;

&lt;p&gt;The difficulty of studying the evolution of language is that in its early forms the available evidences are sparse. Spoken languages don’t leave fossils. Moreover, all existing languages, including the far remote tribal ones, are already sophisticated. Contemporary ones have a lot of words, refined grammar structures and can express almost everything with a remarkable richness of details.  Even in written human records collected so far, dating 5.000 years ago or so, things look almost the same like they are now.
Linguists have studied how communication change over time and inferred how it could appear us when the first rudimental steps toward a language were adopted in the first place.
What are the basic and fundamental aspects and principles of language that whether they would be taken away, the whole towering edifice of language would immediately collapse like a stack of cards?  I would introduce them by a simple composition, which could not be taken as an example of eloquence, but nobody would find it difficult to understand:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;I supermarket enter      basket bring       pick fresh fruit&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;I go cashier       pay cashier basket       bring bag       quit&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As might be noticed, there are no grammatical elements (prepositions, conjunctions, adverbs, plurals, tenses, relative clauses, complement clauses) that glue and hold sentences together, nor any abstract term. Nonetheless, the proto-sentence remains comprehensible due to very few natural principles that arrange those words together. Those principles crystallized into our brain million of years before language was even conceived by our ancestors. The evolution wired those principles in our cortex for facilitating communication.
The first lines of distinction in early languages came from the concrete world, such as actions and things and how to refer to them in space, the pointing words. The second principle refers to the sequentiality of events and and as one can correctly imagine this affect the ordering of words. The third is more about the economy of communication, by contextualizing meanings and references in the sentence.&lt;/p&gt;

&lt;h2 id=&quot;pointing-words&quot;&gt;Pointing words&lt;/h2&gt;
&lt;p&gt;Pointing words assist for referring or locating something in space. They are &lt;em&gt;This&lt;/em&gt;, &lt;em&gt;that&lt;/em&gt;, &lt;em&gt;here&lt;/em&gt;, &lt;em&gt;there&lt;/em&gt; and their reference depends on where the actors are. What is &lt;em&gt;this&lt;/em&gt; for me could be &lt;em&gt;that&lt;/em&gt; for you, due to the relative position of object and subject. Those referencing words are not simply compelling because children use them as an accompaniment to the pointing gesture, reinforcing the intimate link between physical world and mental representation in premature brains. Pointing words, oppositely to other grammatical terms, are not originated by anything else than pointing words. They are root and core concepts.&lt;/p&gt;

&lt;h2 id=&quot;things-actions&quot;&gt;Things, actions&lt;/h2&gt;
&lt;p&gt;The sample text should help to inform that early languages were restricted to simple words, the ones involving only concrete entities in the here and now. Things and action distinction is also a part of what is social intelligence and the world representation which is common in other primates and this conceptual distinction was already there. Even metaphors, that count a large belonging among words of our dictionary, turns out of have concrete origins, they were evolved from elements of physical environment.&lt;/p&gt;

&lt;h2 id=&quot;order-of-words&quot;&gt;Order of words&lt;/h2&gt;
&lt;p&gt;Another basic principle of any language relies on a single strategy: the ordering of words.
What belongs together in reality appears close also in the language and follows the same sequentiality. It is natural to describe an action as central word between two participants. Between the actor and the patient (whom the action is performed) the order is the ordinary mapping from reality to language. Consider for example the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Veni,_vidi,_vici&quot;&gt;Caesar’s Principle&lt;/a&gt;: I came, I saw, I conquered (veni vidi vici). This saying was conferred to Julius Caesar after a victory. The order of words is clearly not accidental, it reflects the sequence of actions in the real world.&lt;/p&gt;

&lt;h2 id=&quot;context&quot;&gt;Context&lt;/h2&gt;
&lt;p&gt;The third principle is concerned with repetition. What is already stated or it is not particularly important does not need to be iterated again. What could be understood and inferred from the context may be omitted in the sentence. This follow the principle of least effort, which is also applicable in language. Whether I would have written the story like this:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;I supermarket enter      I bring basket      I pick fruit      I quit&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;the redundancy of the subject would be truly annoying, in any language. Have been invented several ways to keeping track of participants in the conversation, take by example pronouns.&lt;/p&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html">What is this exhilarating noise come out of my mouth when I talk? Not surely because that precise sequence of sounds, pops and squeezes are particularly melodic, but thanks to that palace of sophistications erected in favor of language, we can talk and afford a wide range of expressions. Since I began erratically to explore natural language processing I have been wondering how it comes out so natural for us, while it is extremely complicated from a computational perspective. What has caught my curiosity is the nature of language and its fundamental aspects that might have shaped the rudimentary ‘Me Tarzan, you Jane’, the sentence that paraphrases the earliest and the simplest level of language.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/hand.jpg" /></entry><entry><title type="html">Concept Search by Word Embeddings</title><link href="http://localhost:4000/2018/06/06/concept-search-by-word-embeddings/" rel="alternate" type="text/html" title="Concept Search by Word Embeddings" /><published>2018-06-06T00:00:00+00:00</published><updated>2018-06-06T00:00:00+00:00</updated><id>http://localhost:4000/2018/06/06/concept-search-by-word-embeddings</id><content type="html" xml:base="http://localhost:4000/2018/06/06/concept-search-by-word-embeddings/">&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/semantic-search-wines.png&quot; alt=&quot;Semantic Search&quot; /&gt;
Catalog search is one of the most important factor to the success of e-commerce sites and accurate and relevant results are critical to successful conversion.&lt;br /&gt;
The following approach aims to reduce user frustration by presenting related products, when searched items are not available in catalog. The central hypothesis is that an user might buy products with similar characteristics of a product originally searched, leading the successful search into a purchase.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://labs.hybris.com/2018/06/11/concept-search-by-word-embeddings/&quot;&gt;see full article&lt;/a&gt;&lt;/p&gt;

&lt;!--
Search engines help to find relevant matches against a query according to various information-retrieval algorithms. Those systems find text occurrences, but regardless their effectiveness, they are unequivocally related to the terms provided by the catalog. Therefore, products cannot be retrieved by words that are not already present in the inventory.

Concept matching (a sub-domain of semantic search) refers to the quality of retrieved instances based on significance. The association of terms by an acceptable grade of relatedness, pivots around those key points:
1. Knowledge gathering. Where is it possible to identify semantic relations among words?
2. Concept extraction. How relations could be extracted and then predicted?

The elaboration applied to the data for obtaining our demanded features is called word embedding.

[Word embedding](https://en.wikipedia.org/wiki/Word_embedding) is a very popular term undoubtedly because of the contribution of the deep learning community. It is associate to the research of [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics), the branch of studies for elaborating semantic similarities between words based on their distributional properties.
&gt; &quot;a word is characterized by the company it keeps&quot;.  cit *R. Firth*

Algorithms (like the well-known [skip-gram](https://en.wikipedia.org/wiki/N-gram#Skip-gram), [cbow](https://en.wikipedia.org/wiki/Bag-of-words_model#CBOW), [glove](https://www.aclweb.org/anthology/D14-1162)) are employed to train models for predict words as they sequentially appears in a given text corpora.  As result, the word embedding model converts a single word into a list of similarities, a vector. Analogous words are represented by similar vectors and [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) measures the cosine of the angle between word vectors, thus scoring the relatedness between two words.

## Concept Matching Algorithm

![Concept Search Diagram](http://localhost:4000/assets/semantic-search-word-embeddings.png)

In the example above the user submits the unknown search query _Chardonnay_ which has some similar terms retrieved in the word embeddings. Some of them might exist in catalog and they are returned to the user.

&lt;small&gt;
**algorithm** *retrieve_alternatives* **is**

&lt;small&gt;
&amp;nbsp;&amp;nbsp;**input**: unrecognized term *query*, word vectors *embeddings*

&lt;small&gt;
&amp;nbsp;&amp;nbsp;**output**: ordered list of products and ranking

&lt;small&gt;
&amp;nbsp;&amp;nbsp;*query_embeddings* ← get similarities of *query* from *embeddings*

&lt;small&gt;
&amp;nbsp;&amp;nbsp;*results* ← empty

&lt;small&gt;
&amp;nbsp;&amp;nbsp;**for each** *w* in *query_embeddings*:

&lt;small&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;*result* ←**search by** *w*

&lt;small&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;*result*.ranking ←result.ranking * w.score

&lt;small&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**append** result to *results*

&lt;small&gt;
&amp;nbsp;&amp;nbsp;**return** *results* **sort by** ranking

## Topic-specific Embeddings

Word embeddings are obtained by elaborating a huge quantity of text, namely _corpus_ or _corpora_. There are available several large and structured set of texts for creating word embeddings: Google News corpus, Wikipedia, and so on, as well as word vectors already trained against those corpora.
Since the quality of word embeddings reflects the corpus from which it has been generated, I purposely created a topic-specific corpora specialized in food, by scanning more than **600** food blogs and collecting roughly **40 Mb** of prepared text. The amount of text is risible in comparison with Google News but nonetheless it is enough for the purposes of computing similarity in the small range of catalog queries. The preparation of corpora includes the remotion of everything but words, case conversion and sentence tokenization. I choose [fastText](https://fasttext.cc/) for elaborating text representations, it uses sub-word information to build vectors for unknown words and as the name might suggest, it is really fast.

This solution has been filed as _&quot;System, computer-implemented method and computer program product for information retrieval&quot;_ at the European patent office. It is applicable to many different domains, like in clothing, automobile, electronics retail, just by getting the proper specialized corpora from which word similarity can be inferred.
--&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html">Catalog search is one of the most important factor to the success of e-commerce sites and accurate and relevant results are critical to successful conversion. The following approach aims to reduce user frustration by presenting related products, when searched items are not available in catalog. The central hypothesis is that an user might buy products with similar characteristics of a product originally searched, leading the successful search into a purchase.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/concept-search-word-embeddings.png" /></entry><entry><title type="html">Stochastic Conversational Workflows</title><link href="http://localhost:4000/2018/03/14/stochastic-conversational-workflows/" rel="alternate" type="text/html" title="Stochastic Conversational Workflows" /><published>2018-03-14T00:00:00+00:00</published><updated>2018-03-14T00:00:00+00:00</updated><id>http://localhost:4000/2018/03/14/stochastic-conversational-workflows</id><content type="html" xml:base="http://localhost:4000/2018/03/14/stochastic-conversational-workflows/">&lt;p&gt;Traditionally, user interfaces are a series of screens and forms for exchanging informations with the user. Most of the applications start with a main screen from which users can navigate using breadcrumbs, menus, buttons like back and forward. This paradigm remained almost unaltered with the coming of hypertext where one may jump from a page or dialog into another by visual links, that are immediately accessible.
Chatbots shift UX towards &lt;em&gt;conversational hypertext&lt;/em&gt; that produces the appearance of having a conversation with the computer. People can interact naturally, and since everyone already knows at least one natural language, nobody needs any training for it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://labs.hybris.com/2018/03/20/stochastic-conversational-workflows/&quot;&gt;see full article&lt;/a&gt;&lt;/p&gt;

&lt;!--

The casualness of the medium contrasts with the complexity of the structured, and sometimes cumbersome, functions for achieving a specific goal, but while web-users can easily switch to a new goal-oriented scenario with a click, in the messaging application this could be mainly done by texting.
Conversational applications usually implement workflows not by screens or forms but by piling new dialog scenarios into the conversational stack. Technically, it is something like packing [finite state machines](https://en.wikipedia.org/wiki/Finite-state_machine), on which every layer represents a particular task. When the current one is accomplished, the dialog state closes and it is removed from the stack.
Conversational workflows may be managed by state-machine engines implemented directly in the chatbot or alternatively by existing flow managers such as Dialogflow (former API.AI), Wit.ai, LUIS.ai (Language Understanding Intelligent Service) by which designers can setup conversation processes in their web dashboards.

![Conventional Dialog Stack](http://localhost:4000/assets/dialog-stack.png)

Even though it might be tempting to assume users will follow the exact logical sequence of steps defined by the bot's designer, it rarely occurs.
The austere interface of a Chatbot does not constraint users in pre-defined schema, it does not prevent nor discourage them to behave like they naturally do with other people: express a demand, ask information, change search criteria for a product, ask maybe again, and eventually pay or just abort the process.

![Rule-base vs stochastic workflow process](http://localhost:4000/assets/checkout-workflows.png)

People do not communicate in _stacks_. They tend to jump from a subject to another almost in a random way. Users may decide to do something entirely different, no matter how the process flow has been structured. They may ask for questions unrelated to the current procedure, or cancel it and then start over again. It
is natural that humans switch topics during dialogue for whatever reason.

Even though it is easy to reach significant outcomes by using one of the mentioned NLU (Natural Language Understanding) systems with little effort, those modelers pose quickly their limitations due to their rigid characteristics concerning _activation_ and _behavior_. In other words, conversational workflows may need to be hard-coded based on few discriminating features, mainly imposed in a limited set of user intentions. It may be almost impossible to manage programmatically a large amount of features that can affect dialogs. Further, variations of such processes may result in growing complexity, becoming unmanageable over time.

## New conversational model
Real world is a stubborn place. It is complex in ways that resist abstraction and modeling. It notices and reacts to our attempts to affect it. Nor can we hope to examine it objectively by pre-defined rules or by programmed state machines.
&gt; The fundamental aspect can't be ignored is the probabilistic nature of the model that will serve those dialogs.

The complexity of the system increases with the number of variables, leading the conversation to a certain function instead of another. Hence, the obligation to move away from rule-based systems and embrace uncertainty, probability and statistic.

Once the limitations of those approaches are unveiled, we are ready to attain  [context and sequentiality](https://gfrison.com/2018/03/05/conversational-contextualization/) in a completely different way. We should let machine learning do what it can do best: calculate predictions over large amount of input.
&gt; The system should being able to replicate human behavior by learning from real conversation segments.

The model should inquiry the system for getting more informations and take more plausible decisions on what to present to the user. With those premises, machine learning is entering into the game, and it comes with the form of neural networks.

## Text classification
Neural networks are adaptable systems whose ability to learn comes from varying the strength of connections between its artificial neurons. They are basically universal function approximators. I described [neural networks for intention classification](https://gfrison.com/2017/09/01/deeplearning-in-text-classification/)  by using convolutions for grasping the semantics behind user's sentences.

## Additional input
For helping the classifier could be provided, other than the raw text, a set of discrete features bring informations that share some predictive relation with the action to choose. Those informations could be somehow related to text itself, but also they could be contextual to the user profile or to the ongoing marketing campaigns promoted by the merchant.

For example, a particular search request should be directed into a special promotion? The time of the day can affect the query selection because in the night a camomile sells better than black tea? Just feed the classifier with those features, and let ML to do the hard work.   
The new conversational model should not just understand the single intention of a phrase, but elaborate it in the overall context the user has engaged with the Chatbot.

## Time series models
The concept of time series is dependent on the idea that past behavior can be used to predict future behavior. In sequence-based models, the output is not just determined by the last input, like in regression predictive models, but also by its proceedings. This peculiar predictor should have some interesting characteristics. Latests input affect more the final output than the ones far away in time, and those models should be able to _override_, _remove_, _remember_  qualifiers along the sequence.
Long short-term memory (LSTM) units come to our rescue. LSTMs can _remember_ values over arbitrary event series and they are a more sophisticated extension of the recurrent neural networks.

## Multi-task learning (MTL)
Consider a hypothetical recommender that has learned to predict your preferences about cars. A mono-task system could be trained to give a single output that might match the car model with your profile. A multi-task model could be trained to not only return you the model's name, but also the color, the engine type, the accessories. MTL aims to solve simultaneously multiple classification tasks within the same neural network. We can view MTL as a form of transfer learning where commonalities and differences across tasks are exploited to improve the overall learning efficiency. It seems that neural networks _love diversity_: more tasks they learn more accurate are their predictions, compared to training them separately. The conversational engine I created uses multi-task learning (MTL). It not merely returns  a single label, but instead it returns a fine-grained set of parameters that add expressiveness into the behavior of the Chatbot.

## Neural network architecture
My proposition is about to use both (CNN, LSTM) in a fully-connected neural network,
in order to leverage classification qualities of CNN with the sequentiality of LSTM.
That means, a particular meaning of an user's utterance is not considered alone like
the current state of the art of Chatbot classifiers, but it is evaluated in the context
of the conversation.
Fully-connected neural network means that the different layers of the network (CNN, LSTM) are affected by the sames feed-forward and back-propagation iterations.

![Stochastic Conversational Workflows](http://localhost:4000/assets/stochastic-conversational-workflows.png)

While CNN extracts a relevant representation of the user's input, other type of inputs can be feed into the LSTM layer as illustrated. In this way, the meaning of the user's sentence is evaluated within a set a additional parameters that can affect the decision outcome of a particular conversational step.
--&gt;</content><author><name>Giancarlo Frison</name><email>giancarlo@gfrison.com</email></author><summary type="html">Traditionally, user interfaces are a series of screens and forms for exchanging informations with the user. Most of the applications start with a main screen from which users can navigate using breadcrumbs, menus, buttons like back and forward. This paradigm remained almost unaltered with the coming of hypertext where one may jump from a page or dialog into another by visual links, that are immediately accessible. Chatbots shift UX towards conversational hypertext that produces the appearance of having a conversation with the computer. People can interact naturally, and since everyone already knows at least one natural language, nobody needs any training for it.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/stochastic-conversational-workflows.png" /></entry></feed>