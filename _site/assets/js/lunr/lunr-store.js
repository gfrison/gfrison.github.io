var store = [{
        "title": "Let’s get Scrum",
        "excerpt":" At school times the teacher used to quote an important saying: culture is what remains after we forget the things we studied thorougly. The concept is charming, but at that time the principle was often adopted to forget things even before they were studied.   The saying is also valid for software development methodologies, where the best practices try to teach us the right path to come up with something really good, shaping a product in the most efficient way and with the highest quality. The agile methodologies set few general rules, but the result depends on you, your skills and your team of course, not on the methodology. Scrum doesn’t produce good software products, but if you are smart, it might suggest you some hints helpful to get away with the failure scenario.   What does Scrum say?  It declares that all activities are in a time box and assigns to each team member his own responsibility based on workload estimation, and the activities priority has to be shared with your chief, most of the time the Scrum master. The daily meetings are essential, the team members explain what they did the day before and what they are going to do today and the blocking problems that affect the task development, as well as the estimated time for the new task or its progress update. As in the rugby game from which Scrum took its name, the goal is to get things done. The powerpoint presentations, the docs are internal artifacts but the objective is to get product shipped.   How? By setting objectives for the next iteration (sprint), and incrementally so on with the next. The iterations firstly face the most critical issues and the trivial ones come later, as you are mostly concerned with the software/system architecture and you’d know if such solution overcome the issue, as soon as possible.   As in the rugby game, the project team will be capable of thinking by itself. The coach hasn’t to enforce a defined set of steps to reach those objectives, and as in a real game the team has to learn to handle chaos of requirement changing and with emerging problems (even hoping that Italy will win the next 6 nations).   My considerations  In my scrum practice, I’ve appreciated the time estimation duty for each single task, as discussing with fellows or build a new feature. However, I don’t find it much helpful during the meetings because it’s an information that project managers need to check the process but not useful to the other developing members. The time estimations are closely related to the tasks, so why not to handle them with the issue/bug tracking system, jira for example? You might use it as a time monitor, so the scrum master can automatically obtain all required informations about the development’s progress. The meeting is an opportunity to get together and to make it clear to others where you are, but most importantly, firstly explain your problems. Sharing difficulties among the members and get proper tips back make the team more integrated and helps to overcome matters quickly.   What impresses of the agile methodology practices is the communication approach, the synchronization of the developers and the feedback on a daily basis. Quick stand up meetings in the morning, before the activities start; maybe better with a coffee.  ","categories": [],
        "tags": ["scrum","methodolody","agile","development","team","process","sprint"],
        "url": "http://localhost:4000/2008/02/21/lets-get-scrum/",
        "teaser":null},{
        "title": "Chain of failures on blocking threads",
        "excerpt":" I came back to Milano little time ago and I’ve bumped into an API implementation in this new job. This will be a library that aims to interact with a remote application through a simple text-based protocol. The typical process is a sequence of authorization – session initialization – commands processing – session disposing each of which enclosed in atomic request/respose interaction. The simplest and most immediate approach provides to write the protocol stubs, and manage them through simple methods that elaborate such commands at low level handling tcp sockets and the client/server handshaking with synchronous calls. Sometimes the simplest is the best way, but not this time, especially within multi layer structured systems, where every component depends on many others, and any of those can fail. This task rings as an alarm bell to me due to a recent project that looked like this one, and I can still remember the effects of hangs and missed responses in a SOA context; fortunately the event happened during a load test:     The application was a client interacting with openfire through XMPP. The investigation uncovered a bug that caused a dead lock in a connection pool in certain conditions, the consequences were easily predictable as the fast resource exhaustion, causing soon an application break down. The application server was over but also the client side was unrecoverable since the unresilient application’s architecture didn’t foresee hang requests.    What is unacceptable is the chain of failures that a problem like this can disseminate along the process path, what about combined systems where one side does not expect the other side to hang off if it stops responding? Domino is a pleasant show, you watch all pieces tracing doodles during their falls, it’s funny but only when it doesn’t look like your system when it works.   Don’t play domino, be skeptical (and use concurrent package)  Blocking threads may happen every time you attempt to get resources out of a connection pool, deal with caches or registred objects, or make calls to external systems as this unfortunate experience above. I mean to be distrustful of each component you inquiry decoupling systems as necessary as to skirt the failure propagation. If your component is properly protected from its neighbours the probability of failure clearly drops down . What does this mean in practice? If you’re dealing with sockets you’re unaware of peer status, except when you send or receive bytes, then check the connectivity polling with fake sends and using setSoTimeout(int timeout) to prevent blocking reads. However, I find much more effective isolating the whole business unit in a single timeboxed job, because delays may also come from huge responses as unbounded result set or file fecthing. If you allow the clients to set timeouts, the request thread quit the operation when the call is not completed in time. Easy? Concurrent programming is hard and it requires high skills and it is even discoraged unless you don’t want to reinvent the wheel. The java.util.concurrent package helps to craft your code with timeout controls as in the following example where I’m encapsulating a job unit (a login) into an ExecutorService.   public class Login implements Callable {  The Login action implements the Callable interface; despite Runnable it may throw checked exceptions when executed.   Login login = new Login(user, password); Future&lt;?&gt; res = exe.submit(login); try {   res.get(commandTimeout, TimeUnit.MILLISECONDS); } catch (ExecutionException e) {   log.error(\"error on login\", e); } finally {   res.cancel(true); }  The tip shows to launch the callable through ScheduledThreadPoolExecutor.submit and waiting the task’s end through Future.get(long timeout, TimeUnit timeUnit). By specifying the timeout value the operation will be completed in time , otherwise a TimeoutException will be thrown.   N.B.: in this last case when timeout occours the ExecutorService doesn’t seem to take care about the still open thread, so don’t forget to execute Future.cancel(true) in the final statement.  ","categories": [],
        "tags": ["callable,","concurrent,","decoupling,","executorservice,","failure,","java,","openfire,","scheduledthreadpoolexecutor,","service,","SOA,","timeout"],
        "url": "http://localhost:4000/2008/06/11/chain-of-failures-on-blocking-threads/",
        "teaser":null},{
        "title": "7 steps to MDA revolution",
        "excerpt":" I didn’t believe that such a successful project was such a rare event in the IT industry, that’s why I’ve never caught another chance for applying the learned lessons again. I thought that the experience accrued on Model Driven Architecture will be reusable in other circumstances, though I’ve never seen concepts as executable UML or MDA either applied or mentioned in the following commitments I’ve pursued into. The idea of this project wasn’t conceived by external consultants thirsting for selling their cool technology; instead, it was born and grew up just inside the development team. The architecture’s transition had been gradual, and little by little, as new automation scenarios penetrated our excited minds, we moved as many as possible development processes under MDA framework. Despite my early impressions while considering to undertake the project, the upper management embraced it and laid down investments counting on the benefits that this new approach would provide to the development. What is difficult to change is the modus operandi of a 300 employee company that offers banking services and applications, which is engaged in one of the most conservative field in technology and development methodologies by default. It was about a significant jump in the services development and as the PM remarked:     “We are developing as dinosaurs, don’t you know what the hell happened to them?”    the way to MDA was traced.   The issues we faced with the introduction of modeling notions would be defined as practical contingencies rather than theoretic or philosophical reasons, foremost the mess in the business layer. It raised reliance and maintenance weaknesses with time, even security holes that sounded so bad in such a company with a plenty of  banks as customers. The hundreds of cases developed by dozens of engineers turning over throughout the months in the Java development area had reached the critical mass, enough to trigger an explosion/implosion of the whole system. On the other hand, the applications can stand up only by high costs of maintenance and lazy deliveries, due to the difficulties on integrating incoming services with the underlying system. The application layer managed the data flows between clients at the top and feeds and legacy information systems at the bottom. On their way, they affected several mixes by business process rules hard-coded in obscure java classes. Unfortunately, most of those shaking details were lost, because of the policy related to the development, which didn’t claim about missing documentation, and then it was so damned annoying to go back and take over old artifacts for maintenance or updating rules. Only skillful programmers might extricate the balled up code. The critical mass had to drop down and be brought to lower temperatures quickly. New developments and dozens of  incoming features were planned, so a deep refactoring was a must; it can wait no more.   How was the domain layer implementation that popped up the highlighted problems? The developer’s effort was mainly focused on the creation of java classes implementing a Command and defining the service to the framework through an xml descriptor. The input and output of such a command was a raw DOM argument, which was parsed to extract the input data needed by the business transaction, the most part of coding was regarded for parsing and filling the response’s service that was a raw xml document too. I think it isn’t agreeable to put most of the developing efforts merely on managing input/output data and mapping, but this was the daily job. Apply the MDA take time, it was an one year evolution, and it would be summarized with:      XSD barriers. It was necessary to set some boundaries for developers, in order to get a minimum of control over the data flows. Each service had its own formal validation on input/output data, though no restrictions were settled on how implementing the services. Never ever elements or attributes not defined in advance by commitments.   Pojo. Replace the raw document with simple pojo as an argument in the call-back methods; this operation aggregates the formal validation with an easy approach on data manipulation. The binding xml-java isn’t hurdle, it is automatic and many available libraries can accomplish this step.   First hints with EMF. Xsd files are models for xml data, EMF is a MOF java implementation, a general abstraction for writing all sorts of models, I don’t linger over it now, but it represented a jump to the service modeling. EMF is an open source library enclosed in the Eclipse platform easy to use and customizable, it aims to separate the abstract model from the ground.   Choice of technology. The play with EMF opened new horizons on modeling facilities. Hooked by this methodology to design SOA applications I realized EMF is not enough, the UML (which core principles are inherited from MOF) can fit much better with my purpose to design the object model, define the process flow and the user experience. UML offers diagrams that you can join together, static and dynamic model may describe most of application structure and behaviour.   Executable UML. What do you do with this bunch of diagrams if you can’t transform them in real artifacts and plug-in them in your SOA framework? Not so much, keeping UML diagrams without related transformations and executions is merely fine for documentation, not much more than this. At that time the company joined the Rational beta-program and I started to develop Eclipse compliant plugins which leveraged the power of UML2 eclipse implementation.   How to define data mapping? One of the main obstacles encountered was the data mapping between two different structures. It happens when you need to connect two or more components inside a service call, and each of those have different data structures. In this case UML doesn’t provide any help and you have to customize the model with special stereotypes and profiles.   Sequence and state diagrams. Class diagram were used to generate java classes, xsd files, copy cobol. Sequence diagrams on the other hand describe the flow of processes and their business rule, even conditional instructions which may be transformed to bpel or custom service descriptors. State diagram shows its benefits modeling the user experience and the steps to complete an operation, it easily tracks the state of sessions and will be transformed into the MVC system, as well as in whatever rich client forms.   Have your say.  ","categories": [],
        "tags": ["banking,","emf,","executable","uml,","java,","mda,","methodology,","model","driven","architecture,","SOA,","uml,","uml2"],
        "url": "http://localhost:4000/2008/06/26/7-steps-to-mda-revolution/",
        "teaser":null},{
        "title": "MDA on fire off the Shoulder of Orion",
        "excerpt":" I must admit, frustration has increased over the years. I mean, interacting with the computer in terms of boolean, long, void. I’d rather sit on a sofa and describe the program by voice, or better, get into a 3D virtual reality and cook a software like a lunch meal in the kitchen. Playing with spheres, arrows, to design all program doodles. I can’t picture it as a possible scenario in the near future, and like for the science fiction movies, we will need to wait a much longer time compared to what envisaged by movie directors or writers to see a minor part of the technologic developments imagined so far actually implemented. It’s just for the secret ambition to get free from the textual codes that I’m debating about how to easily abstract the definition of information systems. In some ways, I’ve managed to do something related to this, in a restricted domain.   When Model Driven Architecture turn out right  Once during an interview, I was asked why I didn’t apply MDA over all software projects I leaded. The question should be hooked as a point of discussion on MDA misconceptions, and generally, on modeling and UML. Unfortunately these interpretations are hindered by a phenomenon that a famous observer of human events (Mark Twain) revealed, that I repropose it again in IT terms:     People commonly use UML like a drunk uses a lamp post; For support rather than illumination.    The initial costs of an MDA are pretty high, the return of the investments starts at the beginning of the automatic code generation. The models are built on the meta-model basis which defines the semantics of the system. Afterwards, the models are turned into code or other resources ready to be installed into the real system. The initial developing efforts are focused on the meta-model and on transforming tools. The investments are rewarded by the automatic transformations that replace the repetitive coding work of programmers; and deeper is the amortization, the more the investment is profitable. The ROI increases as much as the model instances are built, as well as the simplicity (in other words, easier to implement) of the metamodel and the transformation tools. Where shall I apply this approach for best results? In a real-time video application? In a powerful compression API? I don’t think so. I guess high values of this coefficient would be found into SOA systems. MDA is suitable for service domain, like a banking middleware, where you may amortize the modeling system through hundreds of services, with many data structures and flow descriptors, but conformed to few abstract structures, the meta-model actually.   Models are not code  Do you have to adopt UML for design metamodels? Definitely, not. You may define simple data classes in any textual format. The matter might raise when the metamodel grows in complexity, size and when many aspects of the domain are schematized into. Hence, you have to face UML: either you reinvent it, or you use it. Perhaps tracing circles and arrows would be more embarrassing compared to typing on a keyboard, but it could be necessary when structures become hinged and interrelated. Try to join a class diagram with a sequence diagram, then enclose all in a composite for interaction with external parts, and do it without formal and conventional visual patterns and… let me know! UML is a complete and exhaustive, it’s so generalist it would be applied to describe any software model, but for describing not for coding. Many people confuse UML as a programming language. Wrong. It’s a tool for representing a system, a structure, a flow. As mathematics aims to formulate conjectures among countable entities, UML offers a way to define abstract entities. Such high-level language is a mere conceptual schema, it defines components and services, no programs ready to run.   MDA layers  So what do you do with a picture with bubbles and arrows? Is it enough design some diagrams, push a button and voilà: getting a working system that fit your requirements? Nobody believes it, neither myself. The object model is unaware of the underlying system and of its implicit matters. As we know, the model declares the structure and the behaviour differences between one service and another, for all the remaining aspects the system applies common platform specific beaviors. For instance, whether you want to enclose the service into a transaction you may setup a ‘transaction’ stereotype in your own profile meta-model, which will be transformed into a Java annotation or Xml attribute and then properly interpreted by the server framework. Once the MDA is ready, one designs the models and transforms them into hardcore resources, the mythic code bullets, finally deployed into the server. It would appear as too simple, and in order to keep off prejudicial comments I’ll tell you that the real applications are much more complex than this simple vision. You can’t update code bullets by hand because the changes will be lost next time you generate them, automatically. Many exceptions are to be considered, allowing for example, to update generated code. I’ve used merging API like JMerge, and I found it useful to enrich the code without discontinuity from the model and the generated code.  ","categories": [],
        "tags": ["mda,","mdd,","meta-model,","metamodelling,","model,","modelling,","mof,","roi,","uml"],
        "url": "http://localhost:4000/2008/10/23/model-driven-architecture-on-fire-off-the-shoulder-of-orion/",
        "teaser":null},{
        "title": "Recruiter advisory Explicit lyrics",
        "excerpt":"“I’m a people person, very personable. I absolutely insist on enjoying life. Not so task-oriented. Not a work horse. If you’re looking for a Clydesdale I’m probably not your man. Like I don’t live to work, it’s more the other way around. I work to live. Incidentally, what’s your policy on Columbus Day?”  You, Me and Dupree (2006)   This is the interview every recruiter would want at 17.00 on Fridays, so fast to let you step out soon for the forthcoming weekend, plain and clear in the outcome.   Usually it’s not so an easy job for the Head Hunter, selecting people and finding the right ones to slot into the pending position might be hard. The challenge would be more difficult when they are seeking to recruit through controversial methods, which hardly could achieve the wished result. As human beings, we have the natural tendency to think that our choices are rational, while we underestimate the effects of the undercurrents that, in a way or the other, affect our decisions.  We believe to be steady inside the boat in the middle of the sea, even if we are at the mercy of the weaves. We are prone to get swayed.   Fortunately, the good recruiter studies books deeply and prepare himself in the training workshops to get rid of those diversions, and then finally he can apply scientific methods to his job. Progresses in this field could be checked when they act like CIA agents asking questions as “What will you do when you grow up?”, “When was the last time you were happy?” and again “What are your strengths and weaknesses?” rather than “Tell me about yourself, describe yourself in one word”. But, dear recruiter, I can’t describe myself in one word, unless it’s both hyphenated and a metaphor.   What comes first, if I’m talking with the interviewer for the job, is that I want to check if his expectations match with mines. I’m talking with you to show my professional skills, not to talk about my hobbies, neither about dance nor motorcycling.   All those requests never end to astonish me for their futility, even if those make sense just for HR department, I’m not going to dig into the matter.   Behaviour interview  A much more effective approach is to conduct very structured interviews where the questions are focused on experience, skills and ability rather than vague things.   What recruiters sometimes try to follow is the behaviour interview assertion, which declares that the most accurate predictor of future performance is past performance in similar situations. It would be enough to fright any financial mentor but it finds logical basis for canditates’ evaluation. Perhaps the behaviour of a single man is easier predictable than a stock index. HR specialists claim that with this method of leading interviews, it’s much more difficult to get responses that are untrue to the candidate character, because these should be detailed descriptions of past events, or experiences faced at work.   I agree on the idea that past experiences are indicative on how people react under certain circumstances, but I’d put less emphasis on that, first of all because challenges are always different. Whatever technological issue the company is facing right now, it would be far from any candidate experience, recruiter may figure out something else by the applicant. What someone has done shows the ability to execute; personality is important, intelligence naturally more so, but improvisation remains the key.  Knowledge workers must adapt their knowledge to the situation, but if during the interview the candidate isn’t projected on a real scenario to show his capabilities and past learned lessons, how the recruiter actually could form an objective opinion?   I was rarely asked advices or opinions about real technological matters involved on development,  is that the reason the recruiter doesn’t know much about what the new employee is going to resolve?   Maybe sometimes interviews are not used for hiring people, but just to gather information on candidates, to create statistics on salaries, skills and to estimate how long does it take to search for a special kind of professional in the market, I guess.   Money  Recruiters may discard people based on salary, of course they can; especially if the point is that the people are interchangeable, low cost and easily replaceable like a natural resource. usually this happen when the target candidate is junior.   Salary is a complex issue the more senior the target is. Seniors want to discuss the context of the job before they ask about money. Answering the salary question in a phone screen or in an interview before building rapport drop me to the disappointment, as the recruiter is telling me “We want the cheapest on your position”. That’s ok, but do you want to save money before you know what I have to offer? Or, why are you looking for someone senior?   HRs usually match your CV keywords (better know as buzzwords) with their table axis to define your salary box, framing candidates in a very simple way. Although the salary offer is equals among peers, a fascinating metric highlights that 5% of programmers are 20x more productive than the other 95%. Now, let me know in your opinion which section of this statistic is firstly discarded.   Quiz  I don’t see anything wrong with the interview questions with multiple choices, sometimes they are as funny as filling in crosswords, but some other times these questions upset me for I realized I forgot some exponential functions since the school… Damn!   Although it would be helpful to filter out applicants without a basic education, in several years of work I never had concerned about exponential calculus to strike a business requirement.   What's the bottom printed row of this function?  for(int i=0; i&lt;30; i++){    System.out.println(\"line: \"+i); }  a) line: 29 b) line: 30 c) none of the above  These requests end up by annoying their prospective employees, any company would lose appeal, dropping any willingness to get hired by the company.   This is a newbie question, then the recruiter answered me telling that every technical employee in the firm had filled such a questionnaire. Really? I don’t think any of the experienced programmers I know would waste time crisscrossing questions like that  on a job interview unless they are hopelessly unemployed, and if the hiring manager is looking for an experienced developer, why ask these first-level programming questions? If the recruiter can’t read the resume, why would a hiring manager?  ","categories": [],
        "tags": ["applicants,","Behaviour,","hr,","human","resource,","interview,","recruiter"],
        "url": "http://localhost:4000/2009/03/13/recruiter-advisory-explicit-lyrics/",
        "teaser":null},{
        "title": "Waltzing with the Tech Crunch",
        "excerpt":" The economic crisis we’re currently going through is teaching some lessons to the Western countries, in particular to the Anglo-Saxons, that our grandparents know pretty much, although it seems we’ve forgotten the past years in this  financial bubble. The debt has several pros: allows building, buying, investing and, when properly managed, might ensure a safe return and a fair growth of the economy. However, the debt has an outstanding bad side: it must be paid back.  It might be postponed, rolled over, shifted to other (more or less conscious) subjects, and its dreadful effects would be identified as bankrupt, credit-crunch, real estate bubble and recession.   With a view to the software applications, a similar observation might be rightful in terms of ‘state of health’, which point to a family of properties that the software should have to be easy changeable, so it could respond quickly to the requirements evolution. A software that doesn’t enjoy good health is the one that has become fossilised to the original architecture, keeping it as is as possible, never revisited in the light of technological innovations and functional updates, but just patched with improvised and unconvincing surgery.   It’s suffering what it could be defined as inability to bear the debt built up over time, but in this case we’re not dealing with financial debt, this is the technical debt. Even though the term ‘technical debt’ sounds strange, it’s related to the financial fellow in many ways, and it is widespread in software development. The saying according to which economy is based on credit (debt) finds support also in the software world.   Developers who are reading know well what I am talking about. You’re assigned to work in such XYZ firm from next Monday for at least 3 months, and when you’ll start this new task you’ll be instructed about what to do.   The workout mainly consists of implementing new features on top of the customer’s solid rock application, a very remarkable system built some years ago for serving peculiar needs.   “So far, it has worked well”, the owners said, “you may just make it worse than it is now”. Later on, you have no choice but to agree with them.   Expanding or changing the set of features without re-factoring looks like seeding a crop without ploughing the land before, if the system’s authors didn’t predict such an amendment. The harvest could be lost, couldn’t it?   You’ll be asked to complete your job updating the old system and keeping the structure as it is, avoiding to break the fragile balance among components.   Just for you information, consultants were called few months ago for a similar task. They added such a mess into the code that you have to spend most of your time to figure out what they wanted to do than working effectively on new things. Maybe the customer were disappointed by their way to conduct the development and now it’s your turn.   Thus, in addition to the new enhancements, you should fix what your precursors did.   This subject is hard to handle and quite unpleasant in particular when the customer doesn’t want to hear talking about re-factoring unless it doesn’t delay the delivery, which is almost impossible, so the scheduled task proceeds as expected.   In short, it looks like going for a walk over the broken glasses swearing you won’t be injured.   Using the post’s subject, it looks like getting into debt again for covering the old one, just for adding short term solutions when too many of these have been applied in the past.   Looking back at the past, I’m realising how this kind of intervention is predominant on the amount of works done, that I don’t know how much time I would have to wait unemployed if I wanted to work only into brand new projects…   Sometimes I’d define myself as a debt collector, and I find it uncomfortable as a lawyer or a doctor would feel against a criminal prosecution or a rescue surgery: it’s an exploitation of other’s misfortunes. It might be painful, but we make the customer feel better.  ","categories": [],
        "tags": ["consultant,","credit,","crunch,","debt,","development,","improve,","re-build,","re-factoring,","rebuild,","refactoring,","software,","tech,","technical"],
        "url": "http://localhost:4000/2009/04/15/walzing-with-the-tech-crunch/",
        "teaser":null},{
        "title": "Powered by Apache Mina",
        "excerpt":"The application being discussed has to behave as follows: performs the client authentication, accomplishes request and response operations and forwards notifications asynchronously to the client. The Mina framework fulfills these needs because it was created to be as flexible and easy-fitting as possible in an array of case scenarios. Mina is structured in several layers that briefly can be broken down into: input parsing, execution of concatenated processes, and serialisation of  related responses, if needed. The infrastructure takes care of I/O  quirks, while it lets you write within it your business processes and handles session lifecycles through simple callbacks that you can manage within a  few codes. It is Majestic! You will love it.   In addition, there is a need to look for something that could help me, the author, to write down an application which implements many commands, preferably in a appealing way, where each piece of service could be isolated from the rest of the application.   The demultiplexer is a device with a single input and many outputs. Its role is to select the output line according to context rules. This approach is also implemented within Apache Mina for writing decoders, handlers and encoders. Apache Mina’s demux package includes: DemuxingIoHandler, DemuxingProtocolDecoder and the DemuxingProtocolEncoder.     Filters  Filters are used for several purposes: I/O logging, performance tracking, thread pooling, overload control, blacklists, and so on. In a specific case I once had to configure two filters. One for the user authentication, and the other for thread pooling. Once a user is logged in, the authentication filter removes itself from the client session filter list, while substituting one transforming the raw input into POJOs.   Decoder  The TCP server must implements a proprietary protocol. It is a simple ASCII protocol for exchanging commands. Such command lengths are undefinable. They, however, are a sequence of characters much like SMTP. Therefore, the CumulativeProtocolDecoder class is extended enabling it to gather input through the end of command. It is then left to us to parse of the bytes and create a simple Java Bean. Post the operation, the bean is transferred through the filter chain to be executed.   Handler  One of the IoHandler implementations drew my attention while I was looking for something resembling the Command Pattern. Each message coming from clients means a specific action, and I find it so tedious writing a single Handler that switches operations by the type of the incoming request. An elegant solution is provided by DemuxingIoHandler that posts the requests toward the corresponding handler. The handlers have to implement the MessageHandler, the generic type defined will be the object’s class that the DemuxingIoHandler will submit to the handler, and register themselves invoking  addReceivedMessageHandler(Class&lt;E&gt; type, MessageHandler&lt;? super E&gt; handler) .   The asynchronous nature of Mina allows to handle a huge number of clients by an handful set of threads. A further decoupling between I/O and business logic may be done by the ExecutorFilter, which is in charge of the messages after the NioProcessor.   Encoder  The transformation component works in a reverse way compared to the decoder: it serialises the POJO response coming from the handler process, to the output stream, toward the client. Likewise the handlers, it is possible to delegate its own encoder for each response object, but why not to send the IoBuffer straightly from the handler that elaborates the request? Separation of concerns might be the answer. The handler receives a command, a java bean, then it is processed and the handler returns an object for response, a POJO. It’s up to the encoder to transform the abstract response to a concrete message through the agreed TCP protocol.  ","categories": [],
        "tags": ["apache,","apache","mina,","decoder,","demux,","encoder,","filters,","handler,","mina,","server,","tcp"],
        "url": "http://localhost:4000/2009/04/24/powered-by-apache-mina/",
        "teaser":null},{
        "title": "Cool DSL with Groovy",
        "excerpt":"Domain model design has never been confused with ‘ease’. From the dawn of its conception, generating executable Unified Modeling Language (UML) diagrams meant sweat and frustration. Generating stubs &amp; skeletons, alone, led one into quagmires of Java Enterprise Edition. Yet it is inherently possible — and actually has been for ages — to design a programming language dedicated to solve specific domain problems; effortlessly and quickly.   DSL is not a foreign thing. Developers are steeped in DSL, even if unwittingly. Frameworks are DSL. A macro is also DSL. When you write a function? That too is steeped in DSL. But functions are often a dirty business; un-standarized; quirky, whereby even domain experts face a daunting task when unraveling and then being put to task to update such rapscallion boilerplates (that is if they do not themselves compound the problems). A programmer worth his reputation would find it necessary to isolate and clean up all non specific domain terms within the DSL. This literally means rooting through and setting aside all secondary-in-importance setup code, allowing the domain expert to establish or reestablish the primary foundation code. This is most times done for large money for an end user; the helpless customer.      Given an appropriate DSL that fits their needs, customers could write all of the code that they need themselves, without having to be programmers.    Now let us imagine, or better, take for granted, that a good DSL experience can exists. That given an appropriate and standardized DSL environment, even the customer, with a few simple tools and grasp-of-concept, can self-write code, gracefully, and specific to their needs without the cost and hassle of having to become, or running to, programmers. And that is where Groovy comes in.   What’s Groovy?  It’s Java as it should be. Easy and intuitive, it offers new features unknown to its parent yet (I’m still waiting for release 7), and come up with those benefits that form the basis of the DSLs that we will develop.   A fundamental feature is the MOP, the ability of changing runtime the properties and the behaviour of objects. It allows us to respond to method calls that do not exist in the class, in other word to “pretend” that these methods exist. Another essence to consider is the Closure. It’s the real power of Groovy. The extreme flexibility of this kind of object/method allows us to change its behaviour replacing its delegate class, on the fly.   Closure delegate   Groovy has three variables inside each closure for defining different classes in its scope: this, owner, and delegate. The this variable refers to the enclosing class. The owner variable is the enclosing object of the closure. The delegate variable is the same as the owner, unless that delegate is substituted.   class MyClass {   def closure = {     println \"this class:\"+this.class.name     println \"delegate class:\"+delegate.class.name     def nested = {       println \"owner nested class:\"+owner.class.name     }     nested()   } } def tryme = new MyClass().closure tryme.delegate = this tryme()  //output: //this class:MyClass //delegate class:Script1 //owner nested class:MyClass_closure1  When a closure encounters a method call that it cannot handle itself, it automatically relays the invocation to its owner object. If this fails, it relays the invocation to its delegate. one of the reasons builders work the way they do is because they are able to assign the delegate of a closure to themselves.  ","categories": [],
        "tags": ["boilerplates,","ddd,","domain,","domain","expert,","domain","problems,","domain","specific","language,","dsl,","dsl","experience,","foundation","code,","groovy,","java,","java","enterprise","edition,","mda"],
        "url": "http://localhost:4000/2011/11/18/cool-dsl-with-groovy-2/",
        "teaser":null},{
        "title": "Software architect mistakes",
        "excerpt":"I think that to get up in the morning and brew a good cup of coffee is one of the best way to start the day. You know, the heady fragrance that emanates from the machine-pot, it’s delicious. When it’s ready, pour the coffee into a cup, add some sugar, and finally you got it – end of the coffee making process.   Have you ever thought to design a coffee making process with some diagrams, or doing the same with other banal activities such as taking a shower? Of course not. For other cases less trivial than these, including software project development, a minimal-design work can be quite useful and somewhat needed. Often questions arise; is an architecture design worth the time and effort invested in it? Well, you may answer this question first: Are there risks in the project that could be minimized by an early design activity?   The more ambitious and challenging the project is, the higher the number of risks, and the more difficult it is to complete successfully.   How to identifying risks  The easiest place to start is with requirements, in whatever form they take, and to look for things that seem difficult to achieve. Gathering requirements is fundamental for deciding what to do and how. However, sometimes problems arise at this starting point that lead to the ruination of the project. Some assumptions may underestimate this key phase and shake the architect role to its foundations:   It’s someone else’ responsibility to do requirements  Domains drive the architecture choices, not vice-versa. Requirements can create architecture problems. At the very least, you need to assist the business analysts.   I learn the domain as I write the code; incrementally  While prototyping pieces of software is a way for mitigating engineering risks and figuring out the hardest problems, writing code could be a waste of time for analyzing a domain. Rather, it’s very cost-effective to modelling it in advance.   The requirements are already fully understood by the stakeholders  Clear communication is critical between people and the role of a software architect can be a very difficult one when others don’t understand what you do and why.   Domains are irrelevant to architecture choice  Developers may copy an architecture from a past project. Maybe just following the company standard, but ignoring the motivations behind previous choices. They are more likely to be unaware of the qualities required in the current project.   I already know the requirements  At least the documentation should be in your mind, but designers should use models to amplifying their reasoning abilities and unfold not clearly visible aspects that affect their own risks.  ","categories": [],
        "tags": ["architect,","architecture,","architecture","design,","design,","risk,","software,","software","project","development,","stakeholder"],
        "url": "http://localhost:4000/2011/12/16/software-architect-mistakes/",
        "teaser":null},{
        "title": "Gradle archetype for Spring applications",
        "excerpt":"I am releasing a Gradle archetype useful for creating Java/Groovy applications based on Springframework. Of course, it is not a real archetype because such a creation is not possible. However, with very few steps you can create, edit and deploy an application server. It would be a most accommodating starting point for deployable software projects.   This release is an attempt to mitigate common issues related to development life-cycle phases such as testing, the running of application and deployment in various environments. The archetype leverages upon the flexible building process and on the top-most featured IoC (Inversion of Control) management system.   When creating application modules for linking services through HTTP, JMS or any other connector type, this archetype is refined and can be applied for satisfying these requirements:      Automatic testing, building and continuous integration.   A different configuration for each environment (development, integration, production).   Springframework based system.   Groovy support.   The project consists of:      Utility classes for given Spring context   Grails-like DSL for Spring setup (beans.groovy).   Logging and application configuration properties for each environment (development/integration/production).   Gradle config file.   Why Gradle?   Problems exist using Maven in Groovy projects due to the gmaven plugin, which may indicate that it is not ready for the groovy-user community. Indeed, Gradle works perfectly on Groovy projects. It is so concise and elastic that you don’t have just a building system, you have a programming tool. When a customized behaviours proper plugin cannot be found in the registry, you may add custom tasks by writing groovy code directly to the build.gradle descriptor. Gradle is a swiss army knife for developers.   Getting started      Run   git clone git@github.com:gfrison/proto-app.git myApp  where myApp is the name of your project.     Edit property ‘projectName’ in ‘build.gradle’ with project name.   Add classes, and manage them with spring ‘beans.groovy’.   You are now ready to test, run and deploy your project through a continuous integration system such as Jenkins.   If you have suggestions, or pull requests from Github, myself the author, would be happy to consider them.  ","categories": [],
        "tags": ["archetype,","build,","continuous","integration,","development","integration,","gradle,","groovy,","integration","production,","inversion","of","control,","maven,","proper","plugin,","prototype,","swiss","army","knife"],
        "url": "http://localhost:4000/2012/01/16/gradle-archetype-for-spring-applications/",
        "teaser":null},{
        "title": "SOA example application",
        "excerpt":"   SOA describes a set of patterns for creating loosely coupled, standards-based business-aligned services that, because of the separation of concerns between description, implementation, and binding, provide a new level of flexibility.    Service Oriented Architecture terminology has spread in recent years, at least among people who were involved in most of the Information Technology activities. The guidelines suggested by this methodology are granted as major factors to succeed in different distributable systems domains. Just as the definition is clear and easy to understand, so is its implementation into a real project, being intuitive, concise and elegant.   I have released an application demonstrating how SOA’s principles can be applied into a small project making use of EIP (Enterprise Integration Pattern), IoC (Inversion of Control), and a building tool and scripting language such as Groovy. I analized a simple business case: an entertainment provider who wanted to dispatch rewards and bonuses to some of its customers, depending on customer service’s subscriptions. The process sequence is simple:        It is required to provide an implementation of a RewardsService. The service accepts as input a customer account number and a portfolio containing channels subscriptions. The Customer Status team is currently developing the EligibilityService which accepts the account number as an input.    I set up an infrastructure to write acceptance tests for this first meaningful feature. This is what could be defined as a “walking skeleton,” a prototype with the essential aspect that it could be built, deployed and tested after being easily downloaded from Github.     RewardService is invoked by the client and it calls, in turn, the eligibility service which however, in this case is not  implemented. As many real scenarios expect external services, this proof-of-concept refers the eligibility service to a black-box, where only request/response interface is known.   The unit test simulates the eligibility service behaviors mocking the end-point through the Camel Testing Framework. However, if you want to run the application on your local machine I set up, within a line of code, a faux eligibility service that merely returns a positive response:   def alwaysEligible = {exchange -&gt;   if(exchange){     exchange.getOut().setBody('CUSTOMER_ELIGIBLE')   } } as Processor  The entry point is an HTTP Restful interface built upon the Apache CXF, and is easily set up within few lines in the configuration. CXF is initialized by Spring in this following way:   jaxrs.'server'(id:'restService',   address:'http://${http.host}:${http.port}') {     jaxrs.'serviceBeans'{ ref(bean:'rewardService')} }  Services are connected by Apache Camel. RewardService contains only the reference of the ESB context –  an instance of ProducerTemplate. Such solution allows a complete separation between the linking system and the business services. The Camel context represents  the SOA’s wiring, and is configured through a DSL as in the example below:   from('direct:rewards').to(eligibilityServiceEndpoint)  ","categories": [],
        "tags": ["soa,","groovy,","microservice"],
        "url": "http://localhost:4000/2012/01/25/soa-example-application/",
        "teaser":null},{
        "title": "Machine Comprehension on Chatbots",
        "excerpt":"One of the most demanded feature in chatbots is the ability to automatically provide helpful informations. Users might ask about how to pay the purchases online, how to return an defected item, when the purchase could be delivered or just about the opening hours of a shop.   One way to implement this feature is to train a sentence classifier for a determined set of questions the merchant is willing to answer. The system should be instructed on some examples such as: “which credit card do you accept?”, “How do I pay?”, “which payment do you support?” and so on. This simple technique requires a sequence of manual tasks for every conversational agent, such as set up the training and inference pipeline for questions/answers, or reuse the Natural Language Understanding (NLU) system already adopted by the chatbot, if present.   see full article    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2017/07/08/machine-comprehension-on-chatbots/",
        "teaser":null},{
        "title": "Automated Question Answering using Semantic Networks",
        "excerpt":"I worked recently in a small prototype that combines NLP analysis and semantic datasources for answering simple generic questions, by learning how to get the informations given a fairly small amount of question/answer pairs.   Conversational interactions represents the core of any modern Chatbot and the ability to manage utterances and conversations is the strongest indicator of user’s satisfaction. A natural and spontaneous QA dialogue, as every Chatbot would aim to engage, will attempt to solve 3 fundamental issues:     Classify utterances and extract dependencies between words.   Integrate source of knowledge.   Infer transitive semantics (e.g., reconstructing what it is implied but not written).   see full article    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2017/08/01/automated-question-answering-using-semantic-networks/",
        "teaser":null},{
        "title": "Deeplearning in Text Classification",
        "excerpt":"In the Divine Comedy, Minos is a daemon appointed to guard the entrance of the hell. He listens to the sins of souls and indicates them their destinations by wrapping his tail as many times as the assigned circle. The figure is emblematic of the machine learning classification, where an entity is identified as belonging to a category or to another. Rather than condemning souls to endless pains, the harmless tool I am describing can judge whether an user’s utterance belongs to a specific intention, or to a limited range of emotions. Namely, it can serve intention recognition and sentimental analysis.   In the realm of conversational commerce, the examined sentence could be:   I want to buy some apples and pears   The system recognizes the intention search and presents the results.   Intention prediction is not an untackled problem and the market offers plenty of services. There are many players such as Google (Api.ai), Facebook (Wit.ai), Microsoft (Luis.ai) just for mentioning some of them, but this shouldn’t prevent further explorations in the topic, sometimes with unexpected positive surprises, as shows in the graph.      The test was performed against real data used for training the deployed model of the Chatbot system and the results are relevant for the real working scenario, so no cherry picking in this case. 300 training samples, 56 test samples for 25 classes, these are the dataset’s numbers.   Minos, the text classifier, uses an ensemble of machine learning models. It combines multiple classifiers for getting a good prediction out of utterances submitted to Charly. One of the models is based on Convolutional Neural Networks (CNN).   CNN in NLP   CNN is mostly applied to image recognition thanks to the tollerance on translations (rotations, distortions) and the compositionality principle (entities are composed by its constituents). Admittedly, CNN might appear counter-intuitive at a first approach because text looks very different from images:     The order of the words in text is not as important as the order of the pixel in an image.   Humans percept text sequentially, not in convolutions.   Invariance   Entities like images and texts, should be compared differently. The smallest atomic element in text is the single charater, rather than the word, like the pixel in images. The proportion is more like:   text : char = image : pixel   By this angle of view, the order of characters in sentences is fundamental. Convolutions in text come in form of:   single word =&gt; bi-grams (two adjacent words) =&gt; n-grams   like graphical features   lines , corners =&gt; mouths, eyes =&gt; faces   come out of portraits.   In CNN the pair adjective + object for example, could be recognized invariantly of its position, at the begin or at the end of a sentence, exactly like a face is recognized wherever it is located in the whole picture.   Sequentiality   It might seem more intuitive to apply Recurrent Neural Networks (like LSTM, Attention or Seq2seq) for text classification, due to the sequential nature of RNNs algorithms. I didn’t run any test on them so far, but I would promptly play with TreeLSTM. CNN performs well, and one might say that Essentially, all models are wrong, but some are useful, an essay the fit with the idea that final outcome drives the decisions, and experimental results play an important role.   Word Embeddings   Alike any NLP, in CNN words are replaced by their correspective semantic vector. Most famous are Google word2vec, GloVe and FastText. I decided to make use of ConceptNet Numberbatch that took first place in both subtasks at SemEval 2017 task 2. Moreover, the vector file is very small (~250M) compared to Google News word2vec (~1.5G) and from an engeneering point of view, those numbers matter.   Minos is still experimental and not well tuned, doors are open for improvements. An aspect shouldn’t be ignored on working with CNN is the Catastrofic Forgetting, an annoying phenomenon that ruins irrevocably the entire training.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2017/09/01/deeplearning-in-text-classification/",
        "teaser":null},{
        "title": "Context and Sequentiality in Conversational Applications",
        "excerpt":"Contextual memory in conversational applications plays a central role in any type of interaction between the Chatbot and the user. It is the bidirectional transfer of information where interlocutors are aware of the relational, environmental, and cultural context of the exchange. I will show some examples on how a contextual based system might improve the flow of the dialog.   see full article    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2018/03/05/conversational-contextualization/",
        "teaser":null},{
        "title": "Stochastic Conversational Workflows",
        "excerpt":"Traditionally, user interfaces are a series of screens and forms for exchanging informations with the user. Most of the applications start with a main screen from which users can navigate using breadcrumbs, menus, buttons like back and forward. This paradigm remained almost unaltered with the coming of hypertext where one may jump from a page or dialog into another by visual links, that are immediately accessible. Chatbots shift UX towards conversational hypertext that produces the appearance of having a conversation with the computer. People can interact naturally, and since everyone already knows at least one natural language, nobody needs any training for it.   see full article    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2018/03/14/stochastic-conversational-workflows/",
        "teaser":null},{
        "title": "Concept Search by Word Embeddings",
        "excerpt":" Catalog search is one of the most important factor to the success of e-commerce sites and accurate and relevant results are critical to successful conversion.  The following approach aims to reduce user frustration by presenting related products, when searched items are not available in catalog. The central hypothesis is that an user might buy products with similar characteristics of a product originally searched, leading the successful search into a purchase.   see full article    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2018/06/06/concept-search-by-word-embeddings/",
        "teaser":null},{
        "title": "The Basic Principles of Language",
        "excerpt":"What is this exhilarating noise come out of my mouth when I talk? Not surely because that precise sequence of sounds, pops and squeezes are particularly melodic, but thanks to that palace of sophistications erected in favor of language, we can talk and afford a wide range of expressions. Since I began erratically to explore natural language processing I have been wondering how it comes out so natural for us, while it is extremely complicated from a computational perspective. What has caught my curiosity is the nature of language and its fundamental aspects that might have shaped the rudimentary ‘Me Tarzan, you Jane’, the sentence that paraphrases the earliest and the simplest level of language.   The difficulty of studying the evolution of language is that in its early forms the available evidences are sparse. Spoken languages don’t leave fossils. Moreover, all existing languages, including the far remote tribal ones, are already sophisticated. Contemporary ones have a lot of words, refined grammar structures and can express almost everything with a remarkable richness of details.  Even in written human records collected so far, dating 5.000 years ago or so, things look almost the same like they are now. Linguists have studied how communication change over time and inferred how it could appear us when the first rudimental steps toward a language were adopted in the first place. What are the basic and fundamental aspects and principles of language that whether they would be taken away, the whole towering edifice of language would immediately collapse like a stack of cards?  I would introduce them by a simple composition, which could not be taken as an example of eloquence, but nobody would find it difficult to understand:   I supermarket enter      basket bring       pick fresh fruit   I go cashier       pay cashier basket       bring bag       quit   As might be noticed, there are no grammatical elements (prepositions, conjunctions, adverbs, plurals, tenses, relative clauses, complement clauses) that glue and hold sentences together, nor any abstract term. Nonetheless, the proto-sentence remains comprehensible due to very few natural principles that arrange those words together. Those principles crystallized into our brain million of years before language was even conceived by our ancestors. The evolution wired those principles in our cortex for facilitating communication. The first lines of distinction in early languages came from the concrete world, such as actions and things and how to refer to them in space, the pointing words. The second principle refers to the sequentiality of events and and as one can correctly imagine this affect the ordering of words. The third is more about the economy of communication, by contextualizing meanings and references in the sentence.   Pointing words  Pointing words assist for referring or locating something in space. They are This, that, here, there and their reference depends on where the actors are. What is this for me could be that for you, due to the relative position of object and subject. Those referencing words are not simply compelling because children use them as an accompaniment to the pointing gesture, reinforcing the intimate link between physical world and mental representation in premature brains. Pointing words, oppositely to other grammatical terms, are not originated by anything else than pointing words. They are root and core concepts.   Things, actions  The sample text should help to inform that early languages were restricted to simple words, the ones involving only concrete entities in the here and now. Things and action distinction is also a part of what is social intelligence and the world representation which is common in other primates and this conceptual distinction was already there. Even metaphors, that count a large belonging among words of our dictionary, turns out of have concrete origins, they were evolved from elements of physical environment.   Order of words  Another basic principle of any language relies on a single strategy: the ordering of words. What belongs together in reality appears close also in the language and follows the same sequentiality. It is natural to describe an action as central word between two participants. Between the actor and the patient (whom the action is performed) the order is the ordinary mapping from reality to language. Consider for example the Caesar’s Principle: I came, I saw, I conquered (veni vidi vici). This saying was conferred to Julius Caesar after a victory. The order of words is clearly not accidental, it reflects the sequence of actions in the real world.   Context  The third principle is concerned with repetition. What is already stated or it is not particularly important does not need to be iterated again. What could be understood and inferred from the context may be omitted in the sentence. This follow the principle of least effort, which is also applicable in language. Whether I would have written the story like this:   I supermarket enter      I bring basket      I pick fruit      I quit   the redundancy of the subject would be truly annoying, in any language. Have been invented several ways to keeping track of participants in the conversation, take by example pronouns.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2018/06/13/basic-principles-language/",
        "teaser":null},{
        "title": "First Steps on Evolutionary Systems",
        "excerpt":"   Goal programming attempts to find solutions which possibly satisfy, otherwise violates minimally,  a set of goals. It has been enjoyed in innumerable domains such as engineering, financing or resource allocation. Solutions may include optimal strategies to maximize, for example, a sale’s profit or, on the other hand, to minimize the cost of a purchase under an acceptable threshold.   An optimized plan could be blended as a program defined as an abstract syntax tree (AST):     see full article    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2018/06/28/first-steps-evolutionary/",
        "teaser":null},{
        "title": "Program Induction and Synthesis at ICML 2018",
        "excerpt":" The International Conference on Machine Learning ICML took place this year in Europe, in the beautiful city of Stockholm from 10th to 15th of July. This is one of the two premiere conferences (within NIPS) on Artificial Intelligence research, and the numbers indicate the magnitude of the event: 612 accepted papers out of 2473 submissions, 9 tutorial and 67 workshop sessions on the latest advances in all disciplines of machine learning. One of the most intriguing workshop was about machine intelligence capable of writing software code for complex procedural behavior.   see full article    ","categories": [],
        "tags": ["program","induction,","icml,","machine","learning"],
        "url": "http://localhost:4000/2018/08/02/program-induction-synthesis-icml-2018/",
        "teaser":null},{
        "title": "Una Stella in più",
        "excerpt":"     “Se fossi la prima a morire   non lasciare che il dolore oscuri il tuo cielo.  Sii coraggiosa e modesta nel tuo lutto.  È un cambiamento, non un addio.  Così i morti vivono nei vivi  e tutte quelle piccole cose  raccolte nel viaggio della tua vita,  quelle parole semplici ed umili  dettate dal cuore di una mamma sofferente,  sono una ricchezza da conservare gelosamente.  Nel tuo cuore.”   Mirella Bobbo    Hai affrontato la morte più e più volte. Sei stata offesa nel corpo, condannata ad un letto, ad una carrozzina ed alla solitudine. Mi hai insegnato con parole umili e con la tua caparbietà a non mollare mai.      “La mia sofferenza è silenziosa dentro  dentro nell’anima.  Invece fuori parla  parla con il sorriso dell’amicizia,  e profuma di umili piante e fiori di campo.  La mia sofferenza non cerca la pietà  ma la forza d’animo.  E questa forza mi viene data da persone sofferenti.  Senti che il dolore se diviso  pian piano se ne va  lasciando un’infinita gioia e felicità.“   Mirella Bobbo    Inesorabile è stato il logorio del tempo contro cui a nessuno è concesso rivaleggiare. A poco a poco, ti ha tolto la forza, ma non ha mai spento la fiamma che avevi in te. Sei stata una leonessa, di dolcezza e sensibilità.      “Signore, dammi abbastanza lacrime  per mantenermi umana  i sorrisi per conservarmi ottimista  dammi le sconfitte per mantenermi umile  abbastanza successo per mantenermi fiduciosa  gli amici per fondermi coraggio  i ricordi per darmi conforto  abbastanza pazienza per sostenermi nell’attesa  la speranza per accompagnarmi nell’incertezza.  Aiutami a scoprire i tuoi messaggi  Nella realtà che vivo   triste realtà.  Ti prego, non rendermi mai esigente  da pretendere ciò che io vorrei  ma permettimi di ringraziarti per ciò che tu vorrai donarmi.“   Mirella Bobbo    Mi raccontavi di quando per poco, per davvero poco, sei sfuggita da un riposo senza ritorno, grazie alla tempestiva rianimazione dei medici. Ricordavi il momento di pace, il sollievo dai dolori lancinanti che avevi in testa. Per un attimo hai creduto di essere libera e serena. Per fortuna invece, ci hai regalato molto più tempo di quanto la vita voleva darti.   Sembrava quasi ti svegliassi a momenti, quando ti ho rivista ieri sera. Quel momento di pace e quiete è arrivato e domenica scorsa ti sei assopita ed hai lasciato questo mondo, in pace.      “Se mi sveglierò filo d’erba sarò felice  perché tutto il cielo sarà mio.  Se mi sveglierò alito di vento sarò felice  perché sentirete il mio respiro.  Se mi sveglierò pioggia sarò felice  perché disseterò la terra ed il grano crescerà orgoglioso  per il pane di ogni dí.  Se mi sveglierò in un grande giardino  coperto di fiori profumati, dove i bambini corrono felici  sarò felice  perché sarò finalmente in paradiso.“   Mirella Bobbo    Mi manca la tua gioia nel rivedermi. Il tuo sorriso sarà sempre con noi.   Ovunque tu sia, ti voglio bene mamma.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2018/10/26/una-stella-in-piu/",
        "teaser":null},{
        "title": "First steps in Reinforcement Learning",
        "excerpt":"Reinforcement learning covers a family of algorithms with the purpose of maximize a cumulative reward that an agent can obtain from an environment. It seems like training crows to collect cigarette butts in exchange for peanuts, or paraphrasing an old say, the carrot and stick metaphor for cold algorithms instead of living donkeys.   The agent and environment have not been emphasized vainly, they represent more concretely a vacuum cleaner sweeping your flat, an A/B testing engine for commerce or a driveless car in a crossroad. If you have heard about latest advances in the field, you would have came across of Deepmind’s AlphaZero, by which it is possible, with an affordable set of hardware, to build from scratch the best chess player in the world in just 4 hours.         “Difficulties strengthen the mind, as labor does the body.” ― Lucius Annaeus Seneca    Researches don’t lack of challenges in this field. The most important one comes from the intrinsic nature of RL learning process, which rely solely on the evaluations of its actions. Improvements are driven by just one signal, the reward.      Rewards are often very sparse.    They come after hundred or thousands of steps, exponentially increasing the combination of actions the agent must explore for finding a barely better sequence among of them. If that does not seem arduous, consider also the non-stationary nature of some environments, particularly common in dynamic scenarios where the learning phase resemble pursuing a moving target.   Non-stationary environments  Non-stationary means that the return of an action, performed in the precisely exact conditions of a past experience, might be different from what expected. This is particularly intuitive in the case of multi-agent scenario (MARL), in which the agent plays with one or more other agents that are learning too.   What distinguish RL from other optimization methods  While RL helps on creating agents that can autonomously take decisions, other algorithms attain this goal too, but with different working principles.   Supervised learning could be easily distinguished because it is trained with correct samples instead of vague rewards, making it simpler for loss functions to converge into an useful solution.   Mathematical optimization differs from RL in a more subtle way. Likewise RL, the simplex algorithm find solutions by iterating on optimization loops, but it works only on perfect information problems. When considering what it exactly means, let’s look at the knapsack or the traveling salesman problems.     All the necessary informations for elaborating the optimal solution are readily there.    In other words, there is no exploration. Conversely, an RL agent is like a probe on an heavenly body, where the assumptions on the environment are nearly absent. The agent needs to figure out autonomously the good and the bad actions only by the feedback from environment, the so called model free learning approach.   Genetic Programming is an evolutionary optimization method that share most of the characteristics of RL, it is iterative, suitable for imperfect information systems due to its stochastic explorative nature. Even the terminology is somehow related. For example, what is the objective function, in GP is named fitness function, just a polyseme. What differentiate it from RL is it’s evolutionary method, I briefly explained in this post.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2018/12/26/first-steps-reiforcement-learning/",
        "teaser":null},{
        "title": "Dynamic Programming",
        "excerpt":"The term dynamic programming has a curious origin. When Richard Bellman late in the 1940s were seeking for a viral definition of his method, his boss was apparently not very inclined on science and in particular on mathematical research, the activities that Bellman was deep into when he formulate his famous equation. Likewise in marketing campaigns, where names and terms are carefully selected for attention teasing, Bellman coined the definition that combines the multi-staging and time-varying of ‘dynamic’ with the optimization archetyping of ‘programming’, coincidentally induced by the fame of Dantzig’s linear programming for mathematical optimization.        Umberto Boccioni - Rissa in Galleria(1910) by Maurizio Abbiateci (CCBY2)   DP describes problems that involve dynamic processes for finding the best decision one after another. The backbone idea rolls around the splitting of a problem till its atomic parts are identified, then trimming those parts with a objective function. Basically, the core concept of recursion. Considering for example the Fibonacci algorithm:     The node fib(2) appear twice in the tree, therefore Fibonacci unveils an overlapping structure. It is eligible to be an DP algorithm. Not all recursive algorithms are inherently overlapping, look at binary tree search, merge and quick sort, for example. They don’t manifest the property of traversing smaller chunk of data more then once, hence they cannot join Dynamic Programming’s family. When the sub-problems are repeated in the problem as whole, and their values are evaluated all over again unless a caching mechanism (memoization) could be displaced for preventing such inefficiency.   Bellman also conceived the principle of optimality according to which an optimal policy should always hands out the optimal decision from any state or action previously done. However the chessboard pieces’ are laid, or however pedestrian are crossing the road or waiting on the platform, the agent will move pieces properly in case of chess and it will skirt traversing people in case of self-driving car. The automatic actor will always follow the best strategy from the first step and thereafter. This is for introducing another property of DP methods, which is the optimal structure.     When a courier has to deliver a packet from  to the destination , he will catch the shortest way highlighted in blue. The optimal decision for completing the journey , in the shortest path problem, includes necessarily the optimal solution . As you may have noticed, The recursion sweats also out of this property where nested sub-problems are always optimal on their way to the final target. The shortest path problem present the optimal sub-structure, which claims that an optimal solution of a problem includes necessarily the optimal solutions of its sub-problems. Not all recursive problems are optimal even in their encapsulated structure, and the counterfactual is given by slightly different dilemma, the longest path problem. The longest way  includes the node , but if we choose to start from , the path is not included in the way starting from .   Dynamic programming in reinforcement learning  The first time DP and RL were mentioned together was by Minsky in the 1961 and it took form of the Bellman equation, because RL problems have expose usually an overlapping and optimal structure, ideal for being solved by DP.  Markov decision process  Reinforcement learning is a class of methods for determining the optimal policy an agent should apply for maximize its return in a given environment. The entities with a role in RL are the state , the mutable conditions an agent experience in an environment, the action  the agent execute in a particular state, and the reward , if any, which score the goodness of action taken in a state. These are the pillars of the markov decision process, the framework that formalizes the policy  as a sequence of steps , aggregated in episodes.     The policy is a mapping from states and related actions, RL tells us how the agent’s policy changes as result of the experience. For finding good policies, we need to estimate how good it is, in terms of future rewards, to be in a particular state.  Value functions  define the expected return when starting from a given state  and following  thereafter. their fundamental property is that they satisfy recursive relationships similar to what we already have seen for dynamic programming. Hence, the idea of DP in RL is the use of value functions to organize the search for good policies.   The Gridworld example                                         Problem: Find all trajectories towards the flags from any cell           Among many examples I may show for explaining what does DP stands for, this very simple GridWorld conveys the idea of an agent displaced in an 2D environment. The reward drives the agent to move toward the flagged corners regardless the position where the agent is located. How the agent could learn the optimal ways? I give you an hint, let’s start from the end, the flagged boxes. In that position you don’t have anymore rewards to gather, the task is brilliantly completed, so we assume the terminal’s state value is is equal to zero.                                         Iteration 1. Terminal state value is zero           From end position, step back and look around. Among the actions you can do, the move that catapult you into the terminal state is more rewarding than others (zero instead of -1). Congratulations! You already have solved a piece of the puzzle. It is just the last action, you still have to solve the rest of the grid. In that position, as in any other cell of the grid, you need to assign a value to the cell where you are in that particular moment, in order to pave the way to the final goal. That value  indicates how good it is to be there. Not all cells carry the same value, some are more valuable than others. The agent, when choosing the next move, will move to the cell with highest value, among the surroundings. The value would not be just the reward you get in there (-1), but the reward plus the average of the values of the cells proximate to you.                                          Iteration 2: set the terminal state value to _zero_           Going backward till the initial position, you see all rewards on the way to the terminal state waiting for you to being picked up. This is the value of your state. You are in the middle of the trajectory towards the end, and the value of your state is equal of the reward in that state plus the sum of the discounted rewards thereafter. Clearly, the agent moves toward the most promising among the surrounding cells, once it has realized their value.  Let’s keep going. The grid starts to unveil the optimal trajectories by just going backward and evaluating what could be the best move, taking the cell with highest value. A pattern is identifiable, something that programmers knows well, the recursion, and simplified as:    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2019/06/18/dynamic-programming/",
        "teaser":null},{
        "title": "Photography Event in Bassano del Grappa",
        "excerpt":"I’ve been since long time fascinated by photography and its power to capture my fantasy and attention. I was pretty delighted to attend artistic initiatives, that by crossing the boundaries of museums and ateliers, they can reach unforeseen spectators in very unconventional settings. The “FuoriSalone” in Milano, for example, is a majestic example of that. Arts, design and springing creativity flourish in areas where people are used to see anything but design masterpieces. Similarly, in one of the cutest towns on the foot of alps, Bassano del Grappa, is going to take place a photography event that will cover the historic center with splendid images.   I’m honored to give a small contribution, by exposing some pictures of mine in a couple of remarkable sites of the town: the main square (Piazza Liberta’) with its solar clock  and Viale dei Martiri, where visitors can admire the view of the mountains and the medieval towers.   San Marco Caffe’  The Caffe’ is located in the main square, and visitors can see a glimpse of pictures even from the Piazza Liberta’.     Our curse is that we are forced to interpret life as a sequence of events and that when we can’t figure out what our particular story is we feel lost somehow.    The quote reflects somehow the experiment I performed during the Faschingsdienstag (Carnival Tuesday) in München:      Les Amis  Along the Viale dei Martiri, a romantic shop with clothing for woman is hosting a couple of photos taken during an open concert in the Nymphemburg Park (München), Serenade im Park, few summers ago:     Any preference on those pictures?  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2019/09/08/photo-exposition-bassano-fotografia/",
        "teaser":null},{
        "title": "Machine Learning and BigData will make Precision Medicine the future of Healthcare",
        "excerpt":"According to an article published by the British Medical Journal in 2013, medical errors are the third cause of death in the US, and  it is prudent assumption that wrong diagnosis would have a strong incidence in errors that can be harmful to patients. 10-15% of diagnoses are wrong and 35% of adults experienced a medical mistake in the past 5 years involving themselves, their family, or friends. Without considering extreme consequences, the chance to receive a false diagnosis is almost certain along the entire span of life.   originally posted on https://cxlabs.sap.com/2019/06/18/data-analytics-for-precision-medicine/    It’s worth taking a moment to consider why these errors happen in the first place. There are more than 12.000 diseases according to the World Health Organization. Never heard of foreign accent syndrome or alien hand syndrome? Many doctors wouldn’t have either, they’re just two of the many new diseases that are constantly emerging. Among the causes of mistakes, there are the variability of how the disease might manifest. There are doctor variables such as expertise. We know also that errors occur either as a result of physicians’ overconfidence. In the current era of machine learning and big data would be attainable to offer decision-support tools to fix the faulty synthesis of information? May as well be inspiring to look on how the automotive industry adopt those technologies for car maintenance.   Modern cars have many features for ensuring assistance and safety to drivers and passengers. The vehicles’ onboard computer make thousands of decisions based on hundreds of sensors, that among their purposes, perform diagnosis by monitoring internal signals for evidence of faults. Terabytes of data are processed under the hood for real-time analytics, and not just for audit the current state of the car, but also for suggesting the best treatment before real problems occur. In the long term, this can reduce the cost burden of maintenance. Could also predictive diagnostic save human lives by detecting diseases early and precisely even before they appear?    This what Prof. Michael Snyder performed to himself, when he combined high-throughput technologies during 14 months of screenings. Why standard medical evaluations examine only a handful biomedical markers when the technology (in the far 2013) measured up to 40.000 elements and potentially learn a lot more? His paper reports more than 3 billion measurements (50TB of data) over infected and healthy states.  Doing so, it is not only possible to identify physiological statuses that deviate from the healthy state but also do it long before the symptoms appear. Exactly like in your car’s monitoring system.   The concept of precision medicine (PM) hinges on one key premise: most diagnostics are designed for the “average patient” as a “one-size-fits-all” approach. But there is no such “average patient” and thus most treatments will be successful for some patients but not for others. Instead, therapies will be shaped to classes of patients on the basis of the differences in people’s genes, molecular information, clinical data, diet, environment, lifestyle and other biomarkers. Prof. Snyder felt that genomics “saved his life” because he found out about the diabetes early and he was able to make lifestyle changes, even though his health insurance premium became “prohibitively expensive”. However, he says the trade-off was worth it.   Precision medicine has emerged as a computationally approach to interpret the big data of pools of biological molecules (omics) and facilitate their application in healthcare. But doesn’t come without critics. One of them regards costs. Complete genomic sequencing became faster and less expensive with the introduction of new methods. However, reimbursement from insurance companies for these targeted screenings is likely to be still an issue. One of the most imposing barriers of PM may be the willingness and motivation of doctors to incorporate routine genomic testing into their daily workflows while the clinical benefits have not been definitively established.   Algorithmically, there is a shift to using informatics methods such as neural networks and advanced aggregative techniques to model complex relationships among patients within gene and biomolecules signatures to facilitate this process. In the survey Trends in Precision Medicine Adoption made by GenomeWeb and sponsored by Oracle, the majority of respondents believe PM brings competitive advantages to their organizations while one of the most significant obstacles in pursuing PM is the lack of the information technology infrastructure to support the initiative. Interpreting the amount of data collected by medical screenings and also by wearable devices (the so-called Digital Biomarkers) for multiple individuals would cause a structural bottleneck, that could be alleviated by a correspondent shift towards streaming and big data processing, a challenge that could be transformed into opportunity by first-class information providers.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2019/09/27/precision-medicine-healthcare/",
        "teaser":null}]
