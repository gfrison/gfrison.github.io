---
layout: post
description: 10% of diagnoses are wrong and 35% of adults experienced a medical mistake in the past 5 years. Machine learning and big data might offer decision-support tools to fix the faulty system.
title: Machine Learning and BigData will make Precision Medicine the future of Healthcare
published: true
image: /assets/dna.jpg
comments: true
tags:
mathjax: true
---
*originally posted on https://cxlabs.sap.com/2019/06/18/data-analytics-for-precision-medicine/*

According to an [article](https://www.bmj.com/content/353/bmj.i2139) published by the British Medical Journal in 2013, medical errors are the third cause of death in the US, and  it is [prudent assumption](https://qualitysafety.bmj.com/content/22/Suppl_2/ii21) that wrong diagnosis would have a strong incidence in errors that can be harmful to patients. 10-15% of diagnoses are wrong and 35% of adults experienced a medical mistake in the past 5 years involving themselves, their family, or friends. Without considering extreme consequences, the chance to receive a false diagnosis is almost certain along the entire span of life.
<center><img src="{{ site.url }}/assets/causes-of-death.png"/></center>
It’s worth taking a moment to consider why these errors happen in the first place. There are more than 12.000 diseases according to the World Health Organization. Never heard of foreign accent syndrome or alien hand syndrome? Many doctors wouldn’t have either, they’re just two of the many new diseases that are constantly emerging. Among the causes of mistakes, there are the variability of how the disease might manifest. There are doctor variables such as expertise. We know also that errors occur either as a result of [physicians’ overconfidence](https://www.amjmed.com/article/S0002-9343%2808%2900040-5/fulltext). In the current era of machine learning and big data would be attainable to offer decision-support tools to fix the faulty synthesis of information? May as well be inspiring to look on how the automotive industry adopt those technologies for car maintenance.

Modern cars have many features for ensuring assistance and safety to drivers and passengers. The vehicles’ onboard computer make thousands of decisions based on hundreds of sensors, that among their purposes, perform diagnosis by monitoring internal signals for evidence of faults. Terabytes of data are processed under the hood for real-time analytics, and not just for audit the current state of the car, but also for suggesting the best treatment before real problems occur. In the long term, this can reduce the cost burden of maintenance. Could also predictive diagnostic save human lives by detecting diseases early and precisely even before they appear?

<center><img src="{{ site.url }}/assets/snyderome.jpg"/></center>
This what Prof. Michael Snyder [performed](https://www.ncbi.nlm.nih.gov/pubmed/22424236) to himself, when he combined high-throughput technologies during 14 months of screenings. Why standard medical evaluations examine only a handful biomedical markers when the technology (in the far 2013) measured up to 40.000 elements and potentially learn a lot more? His paper reports more than 3 billion measurements (50TB of data) over infected and healthy states.  Doing so, it is not only possible to identify physiological statuses that deviate from the healthy state but also do it long before the symptoms appear. Exactly like in your car’s monitoring system.

The concept of precision medicine (PM) hinges on one key premise: most diagnostics are designed for the “average patient” as a “one-size-fits-all" approach. But there is no such “average patient” and thus most treatments will be successful for some patients but not for others. Instead, therapies will be shaped to classes of patients on the basis of the differences in people’s genes, molecular information, clinical data, diet, environment, lifestyle and other biomarkers. Prof. Snyder felt that genomics “saved his life” because he found out about the diabetes early and he was able to make lifestyle changes, even though his health insurance premium became “prohibitively expensive”. However, he says the trade-off was worth it.

Precision medicine has emerged as a computationally approach to interpret the big data of pools of biological molecules (omics) and facilitate their application in healthcare. But doesn’t come without critics. One of them regards costs. Complete genomic sequencing became faster and less expensive with the introduction of new methods. However, reimbursement from insurance companies for these targeted screenings is likely to be still an issue. One of the most imposing barriers of PM may be the willingness and motivation of doctors to incorporate routine genomic testing into their daily workflows while the clinical benefits have not been definitively established.

Algorithmically, there is a shift to using informatics methods such as neural networks and advanced aggregative techniques to model complex relationships among patients within gene and biomolecules signatures to facilitate this process.
In the survey [Trends in Precision Medicine Adoption](https://go.oracle.com/LP=66444?elqCampaignId=95819) made by GenomeWeb and sponsored by Oracle, the majority of respondents believe PM brings competitive advantages to their organizations while one of the most significant obstacles in pursuing PM is the lack of the information technology infrastructure to support the initiative.
Interpreting the amount of data collected by medical screenings and also by wearable devices (the so-called Digital Biomarkers) for multiple individuals would cause a structural bottleneck, that could be alleviated by a correspondent shift towards streaming and big data processing, a challenge that could be transformed into opportunity by first-class information providers.
