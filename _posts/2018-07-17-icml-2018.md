---
layout: post
description:
title: ICML 2018
published: false
permlink: /2018/07/17/icml-2018
image: https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/06/images.jpeg
---

![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/06/images.jpeg)
The International Conference on Machine Learning [ICML](https://icml.cc) took place this year in Europe,
in the beautiful city of Stockholm from 10th to 15th of July.
This is one of the two premiere conferences (within NIPS) on Artificial Intelligence research, and the numbers indicate the magnitude of the event: 612 accepted papers out of 2473 submissions, 9 tutorial and 67 workshop sessions on the latest advances in all disciplines of machine learning.

## Program Induction

Can we teach computers to write code? This is the question that brings out en entire branch of research specialized in program synthesis.
Try to imagine to describe a problem in plain English and getting a routine in your favorite programming language. It is like migrating
the development process based on punch cards in the _'60s_, to high level languages. Let's have a shot with [this sample](https://arxiv.org/abs/1807.03168):
> You are given a number var0. You have to set var2 to 2. If var0-2 is divisible by 3 you have to set var1 to 1,
otherwise you have to set var1 to zero. For each var3 between 1 and var0-1, if var2 is less than var0 you have to, add var3*3+2 to var2, if var0-var2 is greater than or equal to zero and var0-var2 is divisible by 3 add 1 to var1;
otherwise you have to break from the enclosing loop. You have to return var1.

and this is how the generated python code looks like:
{% gist a4e43b1d6833af9c94b00ab97645f9ab %}

Likewise any ML task, a neural network is trained to detect patterns that might be identified as basic programming primitives, such as
conditional operators, loop controls and even [recursions](https://arxiv.org/abs/1704.06611) and used for generating specific program
[trees]({{ site.url }}{% post_url /2018-06-28-first-steps-evolutionary %}).

<center><img title="Program Induction" src="{{ site.url }}/assets/pinduction-schema.png"/></center>

The problem is extremely challenging and the goodness of the generated programs is still very limited by using existing approaches.
One discrepancy in the flow above settle in the different nature of the encoded solution and generated code.
Neural networks are differentiable while source code belongs to the discrete parish, they have rigorous grammars
and are not tolerant to typos and grammatical mistakes.
It is available a [public dataset](https://near.ai/research/naps/) for training your system to learn programs.

An introductory talk on this matter was hold by [Joshua Tenenbaum](https://www.csail.mit.edu/person/joshua-tenenbaum), professor at MIT and contributor of [Bayesian methods](https://www.researchgate.net/publication/2463513_A_Bayesian_Framework_for_Concept_Learning) for computational learning. He is specialized in cognitive sciences and and he strive to grasp the fundamental mechanics to human learning, in order to ultimately transfer them to computer programs. I find it useful to describe the Bayesian approach as a generic framework for every-day life decisions, where possible solutions are weighted according to our observations and past experiences flavored by doses of uncertainty.

[![DreamCoder]({{ site.url }}/assets/dreamcoder.png)](http://www.youtube.com/watch?v=RB78vRUO6X8?t=3389)

The [DreamCoder](https://uclmr.github.io/nampi/extended_abstracts/ellis.pdf) (Ellis 2018) rumbles on the idea of creating a repository of simple problems (and their paired solutions) for being reused on solving more complex ones. It seems inspired by the ways programmers organize their work: building shared subroutines that can be composed to develop more complex procedures. Instead of solving all problems from scratch, it tries to think flexibly and brings what it has already learned.
